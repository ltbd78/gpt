{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0651d0-d25d-469d-b192-4da08c337986",
   "metadata": {},
   "source": [
    "### How to Create Base Image for Components\n",
    "\n",
    "---\n",
    "Skip if unneeded:\n",
    "\n",
    "0. `gcloud init`\n",
    "1. `gcloud config set project <project-name>`\n",
    "2. `gcloud compute ssh --zone us-central1-a <bucket-name> --internal-ip`\n",
    "3. `gcloud auth configure-docker us-central1-docker.pkg.dev` # (https://cloud.google.com/artifact-registry/docs/docker/authentication)\n",
    "---\n",
    "\n",
    "1. `vim requirements1.txt` and paste:\n",
    "```\n",
    "--index-url <can specify different index>\n",
    "kfp\n",
    "```\n",
    "\n",
    "2. `vim requirements2.txt` and paste:\n",
    "```\n",
    "torch\n",
    "torchvision\n",
    "torchaudio\n",
    "\n",
    "```\n",
    "\n",
    "3. `vim Dockerfile` and paste:\n",
    "```\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "FROM us-docker.pkg.dev/vertex-ai/prediction/pytorch-gpu.2-1:latest\n",
    "COPY requirements1.txt requirements2.txt ./\n",
    "RUN python -m pip install --upgrade pip -r requirements1.txt \n",
    "RUN python -m pip install -r requirements2.txt\n",
    "COPY ./gpt/attention.py ./gpt/model.py ./gpt/dataset.py ./\n",
    "# WARNING: if you do `pip install -r r1.txt -r r2.txt` it will the last arg's --index-url for both\n",
    "```\n",
    "\n",
    "4. Run\n",
    "```\n",
    "Docker build --no-cache .\n",
    "Docker tag <image-name> \"<name>:<tag>\"\n",
    "Docker push \"<name>:<tag>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf007f9-e459-4534-8fcb-d43d88f1bca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GCP initialization\n",
    "PROJECT_ID = \"<project-id>\"\n",
    "! gcloud config set project {PROJECT_ID}\n",
    "! gcloud projects describe  $PROJECT_ID\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_URI = \"gs://<bucket-name>\"\n",
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}\n",
    "SERVICE_ACCOUNT = \"<12-digit-number>-compute@developer.gserviceaccount.com\"\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI\n",
    "BASE_IMAGE = \"<name>:<tag>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34dee9-6d09-4dc9-9c78-c9a6e290e261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.dsl import Input, Output, Dataset, Model\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34e835-a730-44df-baf1-cd7385760047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the Vertex AI SDK for your project and bucket\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21abdfd1-654f-4f03-ab00-0902ce6199f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# component definitions\n",
    "@kfp.dsl.component(base_image=BASE_IMAGE) # TODO: replace with lightweight base_image\n",
    "def get_config(config: dict) -> dict:\n",
    "    # TODO: read config from yaml\n",
    "    return config\n",
    "\n",
    "@kfp.dsl.component(base_image=BASE_IMAGE)\n",
    "def get_data(bucket_data_path: str, data: Output[Dataset]):\n",
    "    from google.cloud import storage\n",
    "    assert bucket_data_path[:5] == 'gs://'\n",
    "    s = bucket_data_path.split('/')\n",
    "    bucket_name = s[2]\n",
    "    path = '/'.join(s[3:])\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    blob.download_to_filename(data.path)\n",
    "    \n",
    "\n",
    "@kfp.dsl.component(base_image=BASE_IMAGE)\n",
    "def train_model(config: dict, data: Input[Dataset]):\n",
    "    import torch\n",
    "    \n",
    "    from dataset import CharacterDataset\n",
    "    dataset_shakespeare = CharacterDataset(data.path, seq_len=config['sequence_dim'])\n",
    "    n = int(.9*len(dataset_shakespeare))\n",
    "    data_train = torch.utils.data.Subset(dataset_shakespeare, list(range(0, n)))\n",
    "    data_val = torch.utils.data.Subset(dataset_shakespeare, list(range(n, len(dataset_shakespeare))))\n",
    "    dl_train = torch.utils.data.DataLoader(data_train, batch_size=config['batch_size'], shuffle=True)\n",
    "    dl_val = torch.utils.data.DataLoader(data_val, batch_size=config['batch_size'], shuffle=True)\n",
    "    \n",
    "    from model import GPT\n",
    "    model = GPT(\n",
    "        dataset_shakespeare.vocab_dim,\n",
    "        config['sequence_dim'],\n",
    "        config['embed_dim'],\n",
    "        config['num_heads'],\n",
    "        config['num_layers'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(config['device'])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(model, iters, device):\n",
    "        out = []\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        losses = torch.zeros(iters)\n",
    "        for dataloader in [dl_train, dl_val]:\n",
    "            i = 0\n",
    "            for x, y in dataloader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                logits = model(x)\n",
    "                loss = model.get_loss(logits, y)\n",
    "                losses[i] = loss.item()\n",
    "                i += 1\n",
    "                if i >= iters - 1:\n",
    "                    break\n",
    "            out.append(losses.mean())\n",
    "        model.train()\n",
    "        return out\n",
    "    \n",
    "    model.train()\n",
    "    tenth = config['train_steps']//10\n",
    "    iter_dl_train = iter(dl_train)\n",
    "    for steps in range(config['train_steps']):\n",
    "        x, y = next(iter_dl_train)\n",
    "        x = x.to(config['device'])\n",
    "        y = y.to(config['device'])\n",
    "        logits = model(x)\n",
    "        loss = model.get_loss(logits, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if steps % tenth == 0:\n",
    "            train_loss, val_loss = estimate_loss(model, 100, config['device'])\n",
    "            print(train_loss, val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5572ea6-8ef5-4c9a-ba7a-b16124d0bda9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipeline definition\n",
    "@kfp.dsl.pipeline\n",
    "def pipeline(config: dict, bucket_data_path: str):\n",
    "    task1 = get_config(config=config)\n",
    "    task2 = get_data(bucket_data_path=bucket_data_path)\n",
    "    task3 = (\n",
    "        train_model(config=task1.output, data=task2.outputs['data'])\n",
    "        .set_cpu_limit('4')\n",
    "        .set_memory_limit('16G')\n",
    "        .add_node_selector_constraint('NVIDIA_TESLA_V100') # https://cloud.google.com/compute/docs/gpus#gpus-list\n",
    "        .set_gpu_limit('1') # https://cloud.google.com/vertex-ai/docs/training/configure-compute#gpu-compatibility-table\n",
    "    ) # https://cloud.google.com/vertex-ai/docs/pipelines/machine-types\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fcc694-5ee3-4b90-99de-536d5fd0145f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile the pipeline\n",
    "compiler = kfp.compiler.Compiler()\n",
    "compiler.compile(\n",
    "    pipeline_func=pipeline, package_path=\"gpt.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e0af92-93c0-4798-967c-f2915c481581",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    batch_size = 16, # N\n",
    "    sequence_dim = 10, # L, S\n",
    "    embed_dim = 13, # E\n",
    "    num_heads = 1, # H\n",
    "    num_layers = 1,\n",
    "    dropout = 0.2,\n",
    "    train_steps = 1000,\n",
    "    lr = 1e-3, # learning rate\n",
    "    seed = 78,\n",
    "    device = 'cuda',\n",
    ")\n",
    "# assert embed_dim % num_heads == 0\n",
    "# torch.manual_seed(78) # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d409b-bfd4-4e7a-9de7-8ff395a43b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# send it to as a job to vertex ai\n",
    "# TODO: research https://cloud.google.com/vertex-ai/docs/training/create-custom-job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"gpt\",\n",
    "    template_path=\"gpt.yaml\",\n",
    "    pipeline_root=f\"{BUCKET_URI}/gpt\", # where component outputs are stored during pipeline runs\n",
    "    parameter_values={ # what to pass into kfp.dsl.pipeline arguments\n",
    "        'config': config,\n",
    "        'bucket_data_path': f\"{BUCKET_URI}/data/shakespeare.txt\"\n",
    "    },\n",
    "    enable_caching=False # rerun pipeline tasks each time instead of using cache\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b4e47-7085-40d3-8913-eaa919616324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09d198-11fd-4958-a808-f483a73dc756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to learn more:\n",
    "# https://www.kubeflow.org/docs/components/pipelines/v2/components/containerized-python-components/"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
