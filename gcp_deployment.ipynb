{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0651d0-d25d-469d-b192-4da08c337986",
   "metadata": {},
   "source": [
    "### How to Create Base Image for Components\n",
    "\n",
    "---\n",
    "Skip if unneeded:\n",
    "\n",
    "0. `gcloud init`\n",
    "1. `gcloud config set project <project-name>`\n",
    "2. `gcloud compute ssh --zone us-central1-a <bucket-name> --internal-ip`\n",
    "3. `gcloud auth configure-docker us-central1-docker.pkg.dev` # (https://cloud.google.com/artifact-registry/docs/docker/authentication)\n",
    "---\n",
    "\n",
    "1. `vim requirements1.txt` and paste:\n",
    "```\n",
    "--index-url <can specify different index>\n",
    "kfp\n",
    "google-cloud-aiplatform\n",
    "google-cloud-storage\n",
    "```\n",
    "\n",
    "2. `vim requirements2.txt` and paste:\n",
    "```\n",
    "torch\n",
    "torchvision\n",
    "torchaudio\n",
    "torchserve\n",
    "torch-model-archiver\n",
    "torch-workflow-archiver\n",
    "tiktoken\n",
    "```\n",
    "\n",
    "Versions at time of writing:\n",
    "```raw\n",
    "kfp                                      2.4.0\n",
    "kfp-pipeline-spec                        0.2.2\n",
    "kfp-server-api                           2.0.3\n",
    "torch                                    2.1.1\n",
    "torch-model-archiver                     0.9.0\n",
    "torch-workflow-archiver                  0.2.11\n",
    "torch-xla                                2.0\n",
    "torchaudio                               2.1.1\n",
    "torchserve                               0.9.0\n",
    "torchvision                              0.16.1\n",
    "```\n",
    "\n",
    "\n",
    "3. `vim Dockerfile` and paste:\n",
    "```\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "FROM us-docker.pkg.dev/vertex-ai/prediction/pytorch-gpu.2-1:latest\n",
    "COPY requirements1.txt requirements2.txt ./\n",
    "RUN python -m pip install --upgrade pip -r requirements1.txt \n",
    "RUN python -m pip install -r requirements2.txt\n",
    "COPY ./gpt/*.py ./\n",
    "# WARNING: if you do `pip install -r r1.txt -r r2.txt` it will the last arg's --index-url for both\n",
    "```\n",
    "\n",
    "4. Run\n",
    "```\n",
    "Docker build --no-cache .\n",
    "Docker tag <image-name> \"<name>:<tag>\"\n",
    "Docker push \"<name>:<tag>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf007f9-e459-4534-8fcb-d43d88f1bca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GCP initialization\n",
    "PROJECT_ID = \"<project-id>\"\n",
    "! gcloud config set project {PROJECT_ID}\n",
    "! gcloud projects describe  $PROJECT_ID\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_URI = \"gs://<bucket-name>\"\n",
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}\n",
    "SERVICE_ACCOUNT = \"<12-digit-number>-compute@developer.gserviceaccount.com\"\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI\n",
    "BASE_IMAGE = \"<name>:<tag>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34dee9-6d09-4dc9-9c78-c9a6e290e261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.dsl import Input, Output, Dataset, Model, Artifact # https://www.kubeflow.org/docs/components/pipelines/v2/data-types/artifacts/\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34e835-a730-44df-baf1-cd7385760047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the Vertex AI SDK for your project and bucket\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21abdfd1-654f-4f03-ab00-0902ce6199f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# component definitions\n",
    "# https://www.kubeflow.org/docs/components/pipelines/v2/components/containerized-python-components/\n",
    "\n",
    "@kfp.dsl.component(base_image=BASE_IMAGE) # TODO: replace with lightweight base_image\n",
    "def get_config(config: dict) -> dict:\n",
    "    assert config['embed_dim'] % config['num_heads'] == 0\n",
    "    # TODO: read config from yaml\n",
    "    return config\n",
    "\n",
    "@kfp.dsl.component(base_image=BASE_IMAGE)\n",
    "def get_data(bucket_data_path: str, artifact_data: Output[Dataset]):\n",
    "    from google.cloud import storage\n",
    "    assert bucket_data_path[:5] == 'gs://'\n",
    "    s = bucket_data_path.split('/')\n",
    "    bucket_name = s[2]\n",
    "    path = '/'.join(s[3:]) # gs://lh-sandbox/data/shakespear.txt\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    blob.download_to_filename(artifact_data.path)\n",
    "    \n",
    "@kfp.dsl.component(base_image=BASE_IMAGE)\n",
    "def train_model(config: dict, artifact_data: Input[Dataset], artifact_model_pth: Output[Model]):\n",
    "    import os\n",
    "    import torch\n",
    "    \n",
    "    torch.manual_seed(config['seed']) # for reproducible experiments; but may slow down model\n",
    "    \n",
    "    from dataset import CharacterDataset\n",
    "    dataset_shakespeare = CharacterDataset(artifact_data.path, seq_len=config['sequence_dim'])\n",
    "    n = int(.95*len(dataset_shakespeare))\n",
    "    dataset_train = torch.utils.data.Subset(dataset_shakespeare, list(range(0, n)))\n",
    "    dataset_val = torch.utils.data.Subset(dataset_shakespeare, list(range(n, len(dataset_shakespeare))))\n",
    "    \n",
    "    from model import GPT\n",
    "    model = GPT(\n",
    "        dataset_shakespeare.vocab_dim,\n",
    "        config['sequence_dim'],\n",
    "        config['embed_dim'],\n",
    "        config['num_heads'],\n",
    "        config['num_layers'],\n",
    "        dropout=config['dropout'],\n",
    "        device=config['device'],\n",
    "    )\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
    "    epochs = 10\n",
    "    steps_per_epoch = config['train_steps'] // epochs\n",
    "    print(f'{\"Epoch\":^5} | {\"Train Loss\":^10} | {\"Val Loss\":^10}')\n",
    "    # Pre-training\n",
    "    loss_train, loss_val = model.evaluate([dataset_train, dataset_val], config['batch_size'], steps_per_epoch)\n",
    "    print(f\"{0:^5} | {loss_train:>10.3f} | {loss_val:>10.3f}\")\n",
    "    # Training\n",
    "    for e in range(1, epochs + 1):\n",
    "        model.fit(dataset_train, optimizer, config['batch_size'], steps_per_epoch)\n",
    "        loss_train, loss_val = model.evaluate([dataset_train, dataset_val], config['batch_size'], steps_per_epoch)\n",
    "        print(f\"{e:^5} | {loss_train:>10.3f} | {loss_val:>10.3f}\")\n",
    "    # Save\n",
    "    os.makedirs(artifact_model_pth.path, exist_ok=True)\n",
    "    torch.save(model, artifact_model_pth.path + \"/model.pth\")\n",
    "\n",
    "# https://cloud.google.com/vertex-ai/docs/training/exporting-model-artifacts\n",
    "\n",
    "# @kfp.dsl.container_component\n",
    "# def prep_deployment(artifact_model_pth: Input[Model], artifact_model_mar: Output[Model]):\n",
    "#     # Note: python code doesn't run here\n",
    "#     return kfp.dsl.ContainerSpec(\n",
    "#         image=BASE_IMAGE,\n",
    "#         command=[\n",
    "#             'sh', '-c', '''\n",
    "#             echo $1\\\n",
    "#             && echo $2\\\n",
    "#             && mkdir -p $1\\\n",
    "#             && echo hello world\\ \n",
    "#             && torch-model-archiver --model-name model --version 0.1 --serialized-file $2 --handler deployment_handler.py --export-path $1\n",
    "#             '''\n",
    "#         ],\n",
    "#         args=[str(artifact_model_mar.path), str(artifact_model_pth.path) + '/model.pth']\n",
    "#         # WARNING: invalid arguments will be skipped w/o error message!; $2 becomes $1, $3 becomes $2\n",
    "#     )\n",
    "\n",
    "@kfp.dsl.component(base_image=BASE_IMAGE)\n",
    "def prep_deployment(artifact_model_pth: Input[Model], artifact_model_mar: Output[Model]):\n",
    "    import subprocess\n",
    "    import os\n",
    "    os.makedirs(artifact_model_mar.path, exist_ok=True)\n",
    "    cmd = f\"torch-model-archiver\\\n",
    "        --model-name model\\\n",
    "        --version 0.1\\\n",
    "        --serialized-file {str(artifact_model_pth.path) + '/model.pth'}\\\n",
    "        --handler deployment_handler.py\\\n",
    "        --export-path {str(artifact_model_mar.path)}\"\n",
    "    subprocess.run([cmd], shell=True)\n",
    "\n",
    "@kfp.dsl.component(base_image=BASE_IMAGE)\n",
    "def deploy_model(project_id: str, image: str, artifact_model_mar: Input[Model], artifact_vertex_endpoint: Output[Artifact], artifact_vertex_model: Output[Model]):\n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project_id)\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"gpt\",\n",
    "        artifact_uri=artifact_model_mar.uri,\n",
    "        serving_container_image_uri=image,\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\") # https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types\n",
    "    artifact_vertex_endpoint.uri = endpoint.resource_name\n",
    "    artifact_vertex_model.uri = deployed_model.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b56475-726e-4d20-bdba-92f3da0b5e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_gcp = dict(\n",
    "    project_id = PROJECT_ID,\n",
    "    bucket_data_path = f\"{BUCKET_URI}/data/shakespeare.txt\",\n",
    "    image = BASE_IMAGE,\n",
    "    cpu_limit = '4',\n",
    "    memory_limit = '16G',\n",
    "    node_selector_constraint = 'NVIDIA_TESLA_V100',\n",
    "    gpu_limit = '1',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5572ea6-8ef5-4c9a-ba7a-b16124d0bda9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipeline definition\n",
    "@kfp.dsl.pipeline\n",
    "def pipeline(config_model: dict, bucket_data_path: str, project_id: str, image: str):\n",
    "    task1 = get_config(config=config_model)\n",
    "    task2 = get_data(bucket_data_path=bucket_data_path)\n",
    "    task3 = (\n",
    "        train_model(config=task1.output, artifact_data=task2.outputs['artifact_data'])\n",
    "        .set_cpu_limit('4')\n",
    "        .set_memory_limit('16G')\n",
    "        .add_node_selector_constraint('NVIDIA_TESLA_V100') # https://cloud.google.com/compute/docs/gpus#gpus-list\n",
    "        .set_gpu_limit('1') # https://cloud.google.com/vertex-ai/docs/training/configure-compute#gpu-compatibility-table\n",
    "    ) # https://cloud.google.com/vertex-ai/docs/pipelines/machine-types\n",
    "    task4 = prep_deployment(artifact_model_pth=task3.outputs['artifact_model_pth'])\n",
    "    task5 = deploy_model(\n",
    "        project_id=project_id,\n",
    "        image=image,\n",
    "        artifact_model_mar=task4.outputs['artifact_model_mar']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fcc694-5ee3-4b90-99de-536d5fd0145f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile the pipeline\n",
    "compiler = kfp.compiler.Compiler()\n",
    "compiler.compile(\n",
    "    pipeline_func=pipeline, package_path=\"gpt.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e0af92-93c0-4798-967c-f2915c481581",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_model = dict(\n",
    "    batch_size = 64, # N\n",
    "    sequence_dim = 100, # L, S\n",
    "    embed_dim = 78, # E\n",
    "    num_heads = 13, # H\n",
    "    num_layers = 3,\n",
    "    dropout = 0.2,\n",
    "    train_steps = 5000,\n",
    "    lr = 1e-3, # learning rate\n",
    "    seed = 78,\n",
    "    device = 'cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d409b-bfd4-4e7a-9de7-8ff395a43b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# send it to as a job to vertex ai\n",
    "# TODO: research https://cloud.google.com/vertex-ai/docs/training/create-custom-job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"gpt\",\n",
    "    template_path=\"gpt.yaml\",\n",
    "    pipeline_root=f\"{BUCKET_URI}/gpt\", # where component outputs are stored during pipeline runs\n",
    "    parameter_values={ # what to pass into kfp.dsl.pipeline arguments\n",
    "        'config_model': config_model,\n",
    "        'bucket_data_path': f\"{BUCKET_URI}/data/shakespeare.txt\",\n",
    "        'project_id': PROJECT_ID,\n",
    "        'image': BASE_IMAGE,\n",
    "    },\n",
    "    enable_caching=False # rerun pipeline tasks each time instead of using cache\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b4e47-7085-40d3-8913-eaa919616324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe1fe1-33df-45d1-be7b-51e4f390bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cleanup\n",
    "# job.delete()\n",
    "# ! gsutil rm -rf {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09d198-11fd-4958-a808-f483a73dc756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to learn more:\n",
    "# https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/kfp2_pipeline.ipynb"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
