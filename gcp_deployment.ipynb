{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0651d0-d25d-469d-b192-4da08c337986",
   "metadata": {},
   "source": [
    "### How to Create Base Image for Components\n",
    "\n",
    "---\n",
    "Skip if unneeded:\n",
    "\n",
    "0. `gcloud init`\n",
    "1. `gcloud config set project <project-name>`\n",
    "2. `gcloud compute ssh --zone us-central1-a <bucket-name> --internal-ip`\n",
    "3. `gcloud auth configure-docker us-central1-docker.pkg.dev` # (https://cloud.google.com/artifact-registry/docs/docker/authentication)\n",
    "---\n",
    "\n",
    "1. `vim requirements1.txt` and paste:\n",
    "```\n",
    "--index-url <can specify different index>\n",
    "kfp\n",
    "google-cloud-aiplatform\n",
    "google-cloud-storage\n",
    "```\n",
    "\n",
    "2. `vim requirements2.txt` and paste:\n",
    "```\n",
    "torch\n",
    "torchvision\n",
    "torchaudio\n",
    "```\n",
    "\n",
    "Note; at time of writing kfp is v2.4 and torch is v2.1.\n",
    "\n",
    "3. `vim Dockerfile` and paste:\n",
    "```\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "FROM us-docker.pkg.dev/vertex-ai/prediction/pytorch-gpu.2-1:latest\n",
    "COPY requirements1.txt requirements2.txt ./\n",
    "RUN python -m pip install --upgrade pip -r requirements1.txt \n",
    "RUN python -m pip install -r requirements2.txt\n",
    "COPY ./gpt/attention.py ./gpt/model.py ./gpt/dataset.py ./\n",
    "# WARNING: if you do `pip install -r r1.txt -r r2.txt` it will the last arg's --index-url for both\n",
    "```\n",
    "\n",
    "4. Run\n",
    "```\n",
    "Docker build --no-cache .\n",
    "Docker tag <image-name> \"<name>:<tag>\"\n",
    "Docker push \"<name>:<tag>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf007f9-e459-4534-8fcb-d43d88f1bca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GCP initialization\n",
    "PROJECT_ID = \"<project-id>\"\n",
    "! gcloud config set project {PROJECT_ID}\n",
    "! gcloud projects describe  $PROJECT_ID\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_URI = \"gs://<bucket-name>\"\n",
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}\n",
    "SERVICE_ACCOUNT = \"<12-digit-number>-compute@developer.gserviceaccount.com\"\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI\n",
    "BASE_IMAGE = \"<name>:<tag>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34dee9-6d09-4dc9-9c78-c9a6e290e261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.dsl import Input, Output, Dataset, Model, Artifact # https://www.kubeflow.org/docs/components/pipelines/v2/data-types/artifacts/\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34e835-a730-44df-baf1-cd7385760047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the Vertex AI SDK for your project and bucket\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21abdfd1-654f-4f03-ab00-0902ce6199f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# component definitions\n",
    "# https://www.kubeflow.org/docs/components/pipelines/v2/components/containerized-python-components/\n",
    "\n",
    "@kfp.dsl.component(base_image=BASE_IMAGE) # TODO: replace with lightweight base_image\n",
    "def get_config(config: dict) -> dict:\n",
    "    assert config['embed_dim'] % config['num_heads'] == 0\n",
    "    # TODO: read config from yaml\n",
    "    return config\n",
    "\n",
    "@kfp.dsl.component(base_image=BASE_IMAGE)\n",
    "def get_data(bucket_data_path: str, artifact_data: Output[Dataset]):\n",
    "    from google.cloud import storage\n",
    "    assert bucket_data_path[:5] == 'gs://'\n",
    "    s = bucket_data_path.split('/')\n",
    "    bucket_name = s[2]\n",
    "    path = '/'.join(s[3:])\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    blob.download_to_filename(artifact_data.path)\n",
    "    \n",
    "@kfp.dsl.component(base_image=BASE_IMAGE)\n",
    "def train_model(config: dict, artifact_data: Input[Dataset], artifact_model: Output[Model]):\n",
    "    import os\n",
    "    import torch\n",
    "    \n",
    "    torch.manual_seed(config['seed']) # for reproducible experiments; but may slow down model\n",
    "    \n",
    "    from dataset import CharacterDataset\n",
    "    dataset_shakespeare = CharacterDataset(artifact_data.path, seq_len=config['sequence_dim'])\n",
    "    n = int(.95*len(dataset_shakespeare))\n",
    "    dataset_train = torch.utils.data.Subset(dataset_shakespeare, list(range(0, n)))\n",
    "    dataset_val = torch.utils.data.Subset(dataset_shakespeare, list(range(n, len(dataset_shakespeare))))\n",
    "    \n",
    "    from model import GPT\n",
    "    model = GPT(\n",
    "        dataset_shakespeare.vocab_dim,\n",
    "        config['sequence_dim'],\n",
    "        config['embed_dim'],\n",
    "        config['num_heads'],\n",
    "        config['num_layers'],\n",
    "        dropout=config['dropout'],\n",
    "        device=config['device'],\n",
    "    )\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
    "    epochs = 10\n",
    "    steps_per_epoch = config['train_steps'] // epochs\n",
    "    print(f'{\"Epoch\":^5} | {\"Train Loss\":^10} | {\"Val Loss\":^10}')\n",
    "    # Pre-training\n",
    "    loss_train, loss_val = model.evaluate([dataset_train, dataset_val], config['batch_size'], steps_per_epoch)\n",
    "    print(f\"{0:^5} | {loss_train:>10.3f} | {loss_val:>10.3f}\")\n",
    "    # Training\n",
    "    for e in range(1, epochs + 1):\n",
    "        model.fit(dataset_train, optimizer, config['batch_size'], steps_per_epoch)\n",
    "        loss_train, loss_val = model.evaluate([dataset_train, dataset_val], config['batch_size'], steps_per_epoch)\n",
    "        print(f\"{e:^5} | {loss_train:>10.3f} | {loss_val:>10.3f}\")\n",
    "    # Save\n",
    "    os.makedirs(artifact_model.path, exist_ok=True)\n",
    "    model.save(os.path.join(artifact_model.path, \"model.pth\"))\n",
    "    \n",
    "@kfp.dsl.component(base_image=BASE_IMAGE)\n",
    "def deploy_model(project_id: str, artifact_model: Input[Model], artifact_vertex_endpoint: Output[Artifact], artifact_vertex_model: Output[Model]):\n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project_id)\n",
    "    print(\"artifact_model.uri:\", artifact_model.uri)\n",
    "    print(\"artifact_model.path:\", artifact_model.path)\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"gpt\",\n",
    "        artifact_uri=artifact_model.uri,\n",
    "        serving_container_image_uri=\"us-central1-docker.pkg.dev/wmt-ri-cust-dev/lh-sandbox/python:torch\",\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\") # https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types\n",
    "    artifact_vertex_endpoint.uri = endpoint.resource_name\n",
    "    artifact_vertex_model.uri = deployed_model.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5572ea6-8ef5-4c9a-ba7a-b16124d0bda9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipeline definition\n",
    "@kfp.dsl.pipeline\n",
    "def pipeline(config: dict, bucket_data_path: str, project_id: str):\n",
    "    task1 = get_config(config=config)\n",
    "    task2 = get_data(bucket_data_path=bucket_data_path)\n",
    "    task3 = (\n",
    "        train_model(config=task1.output, artifact_data=task2.outputs['artifact_data'])\n",
    "        .set_cpu_limit('4')\n",
    "        .set_memory_limit('16G')\n",
    "        .add_node_selector_constraint('NVIDIA_TESLA_V100') # https://cloud.google.com/compute/docs/gpus#gpus-list\n",
    "        .set_gpu_limit('1') # https://cloud.google.com/vertex-ai/docs/training/configure-compute#gpu-compatibility-table\n",
    "    ) # https://cloud.google.com/vertex-ai/docs/pipelines/machine-types\n",
    "    task4 = deploy_model(project_id=project_id, artifact_model=task3.outputs['artifact_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fcc694-5ee3-4b90-99de-536d5fd0145f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile the pipeline\n",
    "compiler = kfp.compiler.Compiler()\n",
    "compiler.compile(\n",
    "    pipeline_func=pipeline, package_path=\"gpt.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e0af92-93c0-4798-967c-f2915c481581",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    batch_size = 64, # N\n",
    "    sequence_dim = 100, # L, S\n",
    "    embed_dim = 78, # E\n",
    "    num_heads = 13, # H\n",
    "    num_layers = 3,\n",
    "    dropout = 0.2,\n",
    "    train_steps = 5000,\n",
    "    lr = 1e-3, # learning rate\n",
    "    seed = 78,\n",
    "    device = 'cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d409b-bfd4-4e7a-9de7-8ff395a43b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# send it to as a job to vertex ai\n",
    "# TODO: research https://cloud.google.com/vertex-ai/docs/training/create-custom-job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"gpt\",\n",
    "    template_path=\"gpt.yaml\",\n",
    "    pipeline_root=f\"{BUCKET_URI}/gpt\", # where component outputs are stored during pipeline runs\n",
    "    parameter_values={ # what to pass into kfp.dsl.pipeline arguments\n",
    "        'config': config,\n",
    "        'bucket_data_path': f\"{BUCKET_URI}/data/shakespeare.txt\",\n",
    "        'project_id': PROJECT_ID,\n",
    "    },\n",
    "    enable_caching=False # rerun pipeline tasks each time instead of using cache\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b4e47-7085-40d3-8913-eaa919616324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe1fe1-33df-45d1-be7b-51e4f390bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cleanup\n",
    "# job.delete()\n",
    "# ! gsutil rm -rf {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09d198-11fd-4958-a808-f483a73dc756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to learn more:\n",
    "# https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/kfp2_pipeline.ipynb"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
