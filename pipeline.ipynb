{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059d2115-3bef-46c7-970b-5da9596633c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "`gcloud init`\n",
    "\n",
    "`gcloud compute ssh --zone us-central1-a <bucket-name> --internal-ip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576ab15-122a-44fc-945f-dd9517b759c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP initialization\n",
    "PROJECT_ID = <your-project-id>\n",
    "! gcloud config set project {PROJECT_ID}\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = REPOSITORY_NAME = \"lh-sandbox\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}\n",
    "shell_output = !gcloud auth list\n",
    "SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "! gsutil iam ch service`Account:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI\n",
    "PIPELINE_IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY_NAME}/gpt:latest\"\n",
    "DEPLOYMENT_IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY_NAME}/gpt_deployment:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d91148-34bb-4172-86fd-0b084fc8499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # run once if you need to build images:\n",
    "# # !gcloud auth configure-docker {REGION}-docker.pkg.dev # https://cloud.google.com/artifact-registry/docs/docker/authentication\n",
    "# !docker build --no-cache -t $PIPELINE_IMAGE_URI . \n",
    "# !docker push $PIPELINE_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34dee9-6d09-4dc9-9c78-c9a6e290e261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import kfp\n",
    "import tiktoken_ext.openai_public\n",
    "from kfp.dsl import Input, Output, Dataset, Model, Artifact # https://www.kubeflow.org/docs/components/pipelines/v2/data-types/artifacts/\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c75745-83cd-482a-9967-2d913bc82867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Vertex AI SDK for your project and bucket\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd6be9-bd8e-4b7e-8d7d-6ae42fe57a15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload necessary dataset and artifacts to bucket\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "\n",
    "# upload dataset\n",
    "blob_data = bucket.blob('data/shakespeare.txt')\n",
    "blob_data.upload_from_filename('data/shakespeare.txt')\n",
    "\n",
    "# upload tiktoken config\n",
    "tiktoken_config = tiktoken_ext.openai_public.gpt2() # encoder/decoder definition to parse dataset\n",
    "with open('./model_artifacts/tiktoken_config.pkl', 'wb') as f:\n",
    "    pickle.dump(tiktoken_config, f)\n",
    "blob_tiktoken_config = bucket.blob('model_artifacts/tiktoken_config.pkl')\n",
    "blob_tiktoken_config.upload_from_filename('./model_artifacts/tiktoken_config.pkl')\n",
    "\n",
    "# upload model config\n",
    "model_config = dict(\n",
    "    batch_size = 64, # N\n",
    "    vocab_dim = tiktoken_config['explicit_n_vocab'],\n",
    "    sequence_dim = 100, # L, S\n",
    "    embed_dim = 78, # E\n",
    "    num_heads = 13, # H\n",
    "    num_layers = 4,\n",
    "    dropout = 0.2,\n",
    "    train_steps = 10000,\n",
    "    lr = 1e-3, # learning rate\n",
    "    seed = 78,\n",
    "    device = 'cuda',\n",
    ")\n",
    "os.makedirs('./model_artifacts', exist_ok=True)\n",
    "with open('./model_artifacts/model_config.pkl', 'wb') as f:\n",
    "    pickle.dump(model_config, f)\n",
    "blob_model_config = bucket.blob('model_artifacts/model_config.pkl')\n",
    "blob_model_config.upload_from_filename('./model_artifacts/model_config.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21abdfd1-654f-4f03-ab00-0902ce6199f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# component definitions\n",
    "# https://www.kubeflow.org/docs/components/pipelines/v2/components/containerized-python-components/\n",
    "\n",
    "@kfp.dsl.component(base_image=PIPELINE_IMAGE_URI)\n",
    "def train_model(bucket_name: str, path_model_config: str, path_tiktoken_config: str, path_text_data: str, model_artifacts: Output[Artifact]):\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    import torch\n",
    "    from google.cloud import storage\n",
    "\n",
    "    from model import GPT\n",
    "    from dataset import WordDataset\n",
    "\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    \n",
    "    blob_text_data = bucket.blob(path_text_data)\n",
    "    blob_tiktoken_config = bucket.blob(path_tiktoken_config)\n",
    "    blob_model_config = bucket.blob(path_model_config)\n",
    "\n",
    "    with blob_text_data.open(mode='r') as f:\n",
    "        text_data = f.read()\n",
    "    with blob_tiktoken_config.open(mode='rb') as f:\n",
    "        tiktoken_config = pickle.load(f)\n",
    "    with blob_model_config.open(mode='rb') as f:\n",
    "        model_config = pickle.load(f)\n",
    "        assert model_config['embed_dim'] % model_config['num_heads'] == 0\n",
    "\n",
    "    # Initalize dataset\n",
    "    dataset_shakespeare = WordDataset(text_data, seq_len=model_config['sequence_dim'], tiktoken_config=tiktoken_config)\n",
    "    n = int(.95*len(dataset_shakespeare))\n",
    "    dataset_train = torch.utils.data.Subset(dataset_shakespeare, list(range(0, n)))\n",
    "    dataset_val = torch.utils.data.Subset(dataset_shakespeare, list(range(n, len(dataset_shakespeare))))\n",
    "    \n",
    "    # Initialize model and parameters\n",
    "    torch.manual_seed(model_config['seed']) # for reproducible experiments; but may slow down model\n",
    "    model = GPT(\n",
    "        dataset_shakespeare.vocab_dim,\n",
    "        model_config['sequence_dim'],\n",
    "        model_config['embed_dim'],\n",
    "        model_config['num_heads'],\n",
    "        model_config['num_layers'],\n",
    "        dropout=model_config['dropout'],\n",
    "        device=model_config['device'],\n",
    "    )\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=model_config['lr'])\n",
    "    epochs = 10\n",
    "    steps_per_epoch = model_config['train_steps'] // epochs\n",
    "    \n",
    "    # Training loop\n",
    "    print(f'{\"Epoch\":^5} | {\"Train Loss\":^10} | {\"Val Loss\":^10}')\n",
    "    loss_train, loss_val = model.evaluate([dataset_train, dataset_val], model_config['batch_size'], steps_per_epoch)\n",
    "    print(f\"{0:^5} | {loss_train:>10.3f} | {loss_val:>10.3f}\") # Pre-training Losses\n",
    "    for e in range(1, epochs + 1):\n",
    "        model.fit(dataset_train, optimizer, model_config['batch_size'], steps_per_epoch)\n",
    "        loss_train, loss_val = model.evaluate([dataset_train, dataset_val], model_config['batch_size'], steps_per_epoch)\n",
    "        print(f\"{e:^5} | {loss_train:>10.3f} | {loss_val:>10.3f}\") # Training Losses\n",
    "    \n",
    "    # Save Artifacts\n",
    "    os.makedirs(model_artifacts.path, exist_ok=True)\n",
    "    with open(model_artifacts.path + '/tiktoken_config.pkl', 'wb') as f:\n",
    "        pickle.dump(tiktoken_config, f)\n",
    "    with open(model_artifacts.path + '/model_config.pkl', 'wb') as f:\n",
    "        pickle.dump(model_config, f)\n",
    "    model.save(model_artifacts.path + '/gpt.pth', optimizer_state_dict=optimizer.state_dict())\n",
    "\n",
    "\n",
    "@kfp.dsl.component(base_image=PIPELINE_IMAGE_URI)\n",
    "def deploy_model(project_id: str, deployment_image: str, model_artifacts: Input[Model], vertex_endpoint: Output[Artifact], vertex_model: Output[Model]):\n",
    "    import os\n",
    "    import pickle\n",
    "    import logging\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform.prediction import LocalModel\n",
    "\n",
    "    from gcp_predictor import GPTPredictor\n",
    "\n",
    "\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    \n",
    "    # ERROR: cannot run docker within docker container; build this in deployment.ipynb\n",
    "    # # https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.prediction.LocalModel\n",
    "    # local_model = LocalModel.build_cpr_model(\n",
    "    #     './', # everything here is copied to image\n",
    "    #     deployment_image, # final output image\n",
    "    #     predictor=GPTPredictor,\n",
    "    #     requirements_path=\"./deploy_requirements.txt\",\n",
    "    # )\n",
    "    # local_model.push_image()\n",
    "    \n",
    "    aiplatform.init(project=project_id) #, location=region)\n",
    "    model = aiplatform.Model.upload(\n",
    "        # local_model=local_model,\n",
    "        display_name=\"gpt\",\n",
    "        artifact_uri=model_artifacts.uri,\n",
    "        serving_container_image_uri=deployment_image, # WARNING: make sure image contains up to date source files\n",
    "    )\n",
    "    endpoint = model.deploy(machine_type=\"n1-standard-4\") # https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = model.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5572ea6-8ef5-4c9a-ba7a-b16124d0bda9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipeline definition\n",
    "@kfp.dsl.pipeline\n",
    "def pipeline(project_id: str, bucket_name: str, path_model_config: str, path_tiktoken_config: str, path_text_data: str, deployment_image: str):\n",
    "    # task1 = get_configs(bucket_name=bucket_name, path_model_config=path_model_config, path_tiktoken_config=path_tiktoken_config)\n",
    "    # task2 = get_data(bucket_name=bucket_name, data_path=data_path)\n",
    "    task1 = (\n",
    "        train_model(bucket_name=bucket_name, path_model_config=path_model_config, path_tiktoken_config=path_tiktoken_config, path_text_data=path_text_data)\n",
    "        .set_cpu_limit('4')\n",
    "        .set_memory_limit('16G')\n",
    "        .add_node_selector_constraint('NVIDIA_TESLA_V100') # https://cloud.google.com/compute/docs/gpus#gpus-list\n",
    "        .set_gpu_limit('1') # https://cloud.google.com/vertex-ai/docs/training/configure-compute#gpu-compatibility-table\n",
    "    ) # https://cloud.google.com/vertex-ai/docs/pipelines/machine-types\n",
    "    task2 = deploy_model(\n",
    "        project_id=project_id,\n",
    "        deployment_image=deployment_image,\n",
    "        model_artifacts=task1.outputs['model_artifacts']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fcc694-5ee3-4b90-99de-536d5fd0145f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile the pipeline\n",
    "compiler = kfp.compiler.Compiler()\n",
    "compiler.compile(\n",
    "    pipeline_func=pipeline, package_path=\"gpt.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d409b-bfd4-4e7a-9de7-8ff395a43b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# send it to as a job to vertex ai\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"gpt\",\n",
    "    template_path=\"gpt.yaml\",\n",
    "    pipeline_root=f\"{BUCKET_URI}/gpt\", # where component outputs are stored during pipeline runs\n",
    "    parameter_values={ # what to pass into kfp.dsl.pipeline arguments\n",
    "        'project_id': PROJECT_ID,\n",
    "        'bucket_name': BUCKET_NAME,\n",
    "        'path_model_config': 'model_artifacts/model_config.pkl',\n",
    "        'path_tiktoken_config': 'model_artifacts/tiktoken_config.pkl',\n",
    "        'path_text_data': 'data/shakespeare.txt',\n",
    "        'deployment_image': DEPLOYMENT_IMAGE_URI,\n",
    "    },\n",
    "    enable_caching=False # rerun pipeline tasks each time instead of using cache\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b4e47-7085-40d3-8913-eaa919616324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe1fe1-33df-45d1-be7b-51e4f390bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cleanup\n",
    "# job.delete()\n",
    "# ! gsutil rm -rf {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b7b5a-5831-4dd2-9f0a-80cc13d920e3",
   "metadata": {},
   "source": [
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/kfp2_pipeline.ipynb"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
