{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e7506f-c7ad-42fc-9908-787e9104c124",
   "metadata": {},
   "source": [
    "Special shoutout to the GOAT Karpathy. This repo follows the theoretical concepts introduced in Karpathy's tutorial but adds many enhancements including:\n",
    "- major stylistic refactors\n",
    "- follows closely to Torch's MultiheadAttention implementation\n",
    "- addition of Dataset Class\n",
    "- removal of extra dropout layer in MultiheadAttention\n",
    "- adds live printing that mimics chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b21d2fa-a70a-46c7-8fec-762e613994e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002df69c-a553-460b-a2e5-4a657d4229dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # N\n",
    "sequence_dim = 100 # L, S\n",
    "embed_dim = 78 # E\n",
    "num_heads = 13 # H\n",
    "num_layers = 3\n",
    "dropout = 0.2\n",
    "assert embed_dim % num_heads == 0\n",
    "train_steps = 5000\n",
    "lr = 1e-3 # learning rate\n",
    "torch.manual_seed(78)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3682ba70-72b1-4a84-bd59-bf4fe996cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shakespeare = CharacterDataset('data.txt', seq_len=sequence_dim)\n",
    "\n",
    "# flavor 1 - shuffled split\n",
    "# data_train, data_test = torch.utils.data.random_split(dataset_shakespeare, [.9, .1])\n",
    "\n",
    "# flavor 2 - non-shuffled split\n",
    "n = int(.9*len(dataset_shakespeare))\n",
    "data_train = torch.utils.data.Subset(dataset_shakespeare, list(range(0, n)))\n",
    "data_val = torch.utils.data.Subset(dataset_shakespeare, list(range(n, len(dataset_shakespeare))))\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "dl_val = torch.utils.data.DataLoader(data_val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6f8740-90b0-46b6-af01-ff643bcf1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03eeddc2-17ab-4f98-93e3-d525c2e3b798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "assert x.shape == y.shape\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c77cc01-9835-456e-8aa2-4167373da7dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tent:\n",
      "The blood upon your visage dries; 'tis time\n",
      "It should be look'd to: come.\n",
      "\n",
      "AUFIDIUS:\n",
      "The town \n",
      "---\n",
      "ent:\n",
      "The blood upon your visage dries; 'tis time\n",
      "It should be look'd to: come.\n",
      "\n",
      "AUFIDIUS:\n",
      "The town i\n"
     ]
    }
   ],
   "source": [
    "print([dataset_shakespeare.decode(l) for l in x.numpy()][0])\n",
    "print('---')\n",
    "print([dataset_shakespeare.decode(l) for l in y.numpy()][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5b0e80f-829d-4cb3-b944-2860cb9a7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(dataset_shakespeare.vocab_dim, sequence_dim, embed_dim, num_heads, num_layers, dropout=dropout).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90e66b27-247d-4e36-ae45-c8b14fa71852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding): Embedding(65, 78)\n",
       "  (position_embedding): Embedding(100, 78)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiheadAttention(\n",
       "        (query): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (key): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (value): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (projection): Linear(in_features=78, out_features=78, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiheadAttention(\n",
       "        (query): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (key): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (value): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (projection): Linear(in_features=78, out_features=78, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiheadAttention(\n",
       "        (query): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (key): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (value): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (projection): Linear(in_features=78, out_features=78, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear): Linear(in_features=78, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15e60327-7f9a-4081-8e54-d36f809ed8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hijT?Kj;Wo3pEUI!:fKnM!I.kS\n",
      "gbJS?GIEBvru-ff!afZ?ko?EIIVnI:KfVoKASj-KcFlrwF'iR?I:zG\n",
      "nr\n",
      "osZcJf$RIQKxywgI&"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0, 46, 47, 48, 32, 12, 23, 48, 11, 35, 53,\n",
       "          9, 54, 17, 33, 21,  2, 10, 44, 23, 52, 25,  2, 21,  8, 49, 31,  0, 45,\n",
       "         40, 22, 31, 12, 19, 21, 17, 14, 60, 56, 59,  7, 44, 44,  2, 39, 44, 38,\n",
       "         12, 49, 53, 12, 17, 21, 21, 34, 52, 21, 10, 23, 44, 34, 53, 23, 13, 31,\n",
       "         48,  7, 23, 41, 18, 50, 56, 61, 18,  5, 47, 30, 12, 21, 10, 64, 19,  0,\n",
       "         52, 56,  0, 53, 57, 38, 41, 22, 44,  3, 30, 21, 29, 23, 62, 63, 61, 45,\n",
       "         21,  4],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0, 40, 63, 43, 48, 23, 49,  1, 29, 57, 29,  4,\n",
       "         14, 15,  7, 17, 44, 48, 60, 21, 64, 47, 38, 21, 26,  8,  7, 53,  7, 13,\n",
       "         50, 10, 62, 48, 36,  7, 56, 21, 30, 55, 28,  3, 42,  2, 34, 12,  2, 61,\n",
       "         56, 62, 20, 56, 63, 60, 53, 38, 34, 12, 35, 33, 56, 31,  2, 36, 21, 59,\n",
       "          0, 22, 26, 25, 30, 33, 16, 55, 38, 44,  2, 26, 59, 56, 57, 61, 61,  8,\n",
       "         23, 25, 53, 31, 59,  2, 56, 53, 25, 44, 61, 26,  9, 57,  2,  7, 46, 49,\n",
       "         46, 16]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretraining\n",
    "model.generate(dataset_shakespeare.encode, dataset_shakespeare.decode, ['hi', 'bye'], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "746ba3c0-3f54-42e4-90ca-4bd353b04013",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, iters, device):\n",
    "    out = []\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    losses = torch.zeros(iters)\n",
    "    for dataloader in [dl_train, dl_val]:\n",
    "        i = 0\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = model.get_loss(logits, y)\n",
    "            losses[i] = loss.item()\n",
    "            i += 1\n",
    "            if i >= iters - 1:\n",
    "                break\n",
    "        out.append(losses.mean())\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08852f4a-0be8-495e-a74c-295a5360888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.1428) tensor(4.1478)\n",
      "tensor(2.1975) tensor(2.2194)\n",
      "tensor(1.9136) tensor(1.9856)\n",
      "tensor(1.7711) tensor(1.8973)\n",
      "tensor(1.6899) tensor(1.8454)\n",
      "tensor(1.6362) tensor(1.8026)\n",
      "tensor(1.6046) tensor(1.7686)\n",
      "tensor(1.5774) tensor(1.7571)\n",
      "tensor(1.5595) tensor(1.7369)\n",
      "tensor(1.5407) tensor(1.7182)\n",
      "CPU times: total: 1min 43s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train()\n",
    "tenth = train_steps//10\n",
    "iter_dl_train = iter(dl_train)\n",
    "for steps in range(train_steps):\n",
    "    x, y = next(iter_dl_train)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    logits = model(x)\n",
    "    loss = model.get_loss(logits, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % tenth == 0:\n",
    "        train_loss, val_loss = estimate_loss(model, 100, device)\n",
    "        print(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4874f602-4a11-4864-baed-aef55e0a66f0",
   "metadata": {},
   "source": [
    "# Experiments on 3070 Laptop\n",
    "\n",
    "batch_size = 64 # N\n",
    "sequence_dim = 100 # L, S\n",
    "embed_dim = 78 # E\n",
    "num_heads = 13 # H\n",
    "num_layers = 3\n",
    "dropout = 0.2\n",
    "assert embed_dim % num_heads == 0\n",
    "train_steps = 5000\n",
    "lr = 1e-3 # learning rate\n",
    "torch.manual_seed(78)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "---\n",
    "\n",
    "# attention_slow.py\n",
    "tensor(4.0550) tensor(4.0607)\n",
    "tensor(2.1289) tensor(2.1712)\n",
    "tensor(1.8577) tensor(1.9556)\n",
    "tensor(1.7323) tensor(1.8749)\n",
    "tensor(1.6610) tensor(1.8245)\n",
    "tensor(1.6167) tensor(1.7915)\n",
    "tensor(1.5815) tensor(1.7588)\n",
    "tensor(1.5577) tensor(1.7404)\n",
    "tensor(1.5335) tensor(1.7101)\n",
    "tensor(1.5162) tensor(1.7123)\n",
    "CPU times: total: 1min 35s\n",
    "Wall time: 5min 30s\n",
    "\n",
    "--\n",
    "\n",
    "# attention.py need_weights=True\n",
    "tensor(4.0399) tensor(4.0498)\n",
    "tensor(2.1530) tensor(2.1911)\n",
    "tensor(1.8653) tensor(1.9626)\n",
    "tensor(1.7370) tensor(1.8729)\n",
    "tensor(1.6652) tensor(1.8210)\n",
    "tensor(1.6200) tensor(1.7882)\n",
    "tensor(1.5879) tensor(1.7570)\n",
    "tensor(1.5731) tensor(1.7515)\n",
    "tensor(1.5505) tensor(1.7238)\n",
    "tensor(1.5323) tensor(1.7157)\n",
    "CPU times: total: 16 s\n",
    "Wall time: 1min 55s\n",
    "\n",
    "--\n",
    "\n",
    "# attention.py need_weights=False\n",
    "tensor(4.0481) tensor(4.0581)\n",
    "tensor(2.1748) tensor(2.2093)\n",
    "tensor(1.9199) tensor(2.0003)\n",
    "tensor(1.7921) tensor(1.9143)\n",
    "tensor(1.7106) tensor(1.8671)\n",
    "tensor(1.6642) tensor(1.8268)\n",
    "tensor(1.6316) tensor(1.7987)\n",
    "tensor(1.6027) tensor(1.7774)\n",
    "tensor(1.5774) tensor(1.7513)\n",
    "tensor(1.5602) tensor(1.7437)\n",
    "CPU times: total: 13.2 s\n",
    "Wall time: 1min 38s\n",
    "\n",
    "--\n",
    "\n",
    "## nn.MultiheadAttention need_weights=True\n",
    "tensor(4.0971) tensor(4.1059)\n",
    "tensor(2.1829) tensor(2.2199)\n",
    "tensor(1.9037) tensor(1.9881)\n",
    "tensor(1.7707) tensor(1.9049)\n",
    "tensor(1.6953) tensor(1.8548)\n",
    "tensor(1.6488) tensor(1.8201)\n",
    "tensor(1.6112) tensor(1.7888)\n",
    "tensor(1.5871) tensor(1.7675)\n",
    "tensor(1.5616) tensor(1.7371)\n",
    "tensor(1.5388) tensor(1.7248)\n",
    "CPU times: total: 17 s\n",
    "Wall time: 1min 42s\n",
    "\n",
    "--\n",
    "\n",
    "# nn.MultiheadAttention need_weights=False\n",
    "tensor(4.0971) tensor(4.1059)\n",
    "tensor(2.1824) tensor(2.2192)\n",
    "tensor(1.9042) tensor(1.9884)\n",
    "tensor(1.7707) tensor(1.9026)\n",
    "tensor(1.6937) tensor(1.8532)\n",
    "tensor(1.6474) tensor(1.8209)\n",
    "tensor(1.6112) tensor(1.7916)\n",
    "tensor(1.5873) tensor(1.7727)\n",
    "tensor(1.5572) tensor(1.7368)\n",
    "tensor(1.5367) tensor(1.7277)\n",
    "CPU times: total: 13.6 s\n",
    "Wall time: 1min 40s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a6f0d0f-73ce-4814-9988-1a633b5295d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinsuA:\n",
      "GLOUCES:\n",
      "'Tu!\n",
      "LOWGAGUE:\n",
      "Kll?\n",
      "\n",
      "WAMILLA:\n",
      "KEd VINCENTIO:\n",
      "You shall cleant:\n",
      "Me, no drick me; your trord, let Clifford,\n",
      "In far Turn. Hence world! I'll get.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Thou art first, it you do say him.\n",
      "\n",
      "VOLUMNIA:\n",
      "He have are trurn me your lord.\n",
      "\n",
      "Find Get imposst the let pray the cast,\n",
      "Gave sovere lie, therefore is his vallite\n",
      "Wife an his, shall when norshing you, one by bloodes!\n",
      "Who dow'd I do, as the hose, but he soul bid is face,\n",
      "And blooddy, it his desire to's deithey.\n",
      "Who delied me asd it my queen his so ath\n",
      "Commi'd, and madrers matter to my fear tank's deads.\n",
      "\n",
      "First Secrvents:\n",
      "That is of Signians, From caious bewn hate\n",
      "Which at bane hands lover, but frect the prise?\n",
      "\n",
      "ROMEO:\n",
      "He, be one colduage your of all to him.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "My rise but you hope of your son.\n",
      "\n",
      "ISABELLA:\n",
      "If would be the First, like and my sir.\n",
      "\n",
      "PAMPSOLINUS:\n",
      "Grave 'twill when them us to appace will theem,\n",
      "Whils the wan stome away but where to him, in,\n",
      "Privosabry crown right of is the subje,\n",
      "And alture this "
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  ..., 42,  1, 40],\n",
       "        [ 0,  0,  0,  ..., 47, 57,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# post training\n",
    "model.generate(dataset_shakespeare.encode, dataset_shakespeare.decode, ['Han', 'Linsu'], 1000, print_batch_num=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
