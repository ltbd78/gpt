{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b21d2fa-a70a-46c7-8fec-762e613994e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002df69c-a553-460b-a2e5-4a657d4229dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # N\n",
    "sequence_dim = 100 # L, S\n",
    "embed_dim = 78 # E\n",
    "num_heads = 13 # H\n",
    "num_layers = 3\n",
    "dropout = 0.2\n",
    "assert embed_dim % num_heads == 0\n",
    "train_steps = 5000\n",
    "lr = 1e-3 # learning rate\n",
    "torch.manual_seed(78)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3682ba70-72b1-4a84-bd59-bf4fe996cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shakespeare = CharacterDataset('input.txt', seq_len=sequence_dim)\n",
    "\n",
    "# flavor 1 - shuffled split\n",
    "# data_train, data_test = torch.utils.data.random_split(dataset_shakespeare, [.9, .1])\n",
    "\n",
    "# flavor 2 - non-shuffled split\n",
    "n = int(.9*len(dataset_shakespeare))\n",
    "data_train = torch.utils.data.Subset(dataset_shakespeare, list(range(0, n)))\n",
    "data_val = torch.utils.data.Subset(dataset_shakespeare, list(range(n, len(dataset_shakespeare))))\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "dl_val = torch.utils.data.DataLoader(data_val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6f8740-90b0-46b6-af01-ff643bcf1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03eeddc2-17ab-4f98-93e3-d525c2e3b798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "assert x.shape == y.shape\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c77cc01-9835-456e-8aa2-4167373da7dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tent:\n",
      "The blood upon your visage dries; 'tis time\n",
      "It should be look'd to: come.\n",
      "\n",
      "AUFIDIUS:\n",
      "The town \n",
      "---\n",
      "ent:\n",
      "The blood upon your visage dries; 'tis time\n",
      "It should be look'd to: come.\n",
      "\n",
      "AUFIDIUS:\n",
      "The town i\n"
     ]
    }
   ],
   "source": [
    "print([dataset_shakespeare.decode(l) for l in x.numpy()][0])\n",
    "print('---')\n",
    "print([dataset_shakespeare.decode(l) for l in y.numpy()][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5b0e80f-829d-4cb3-b944-2860cb9a7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(dataset_shakespeare.vocab_dim, sequence_dim, embed_dim, num_heads, num_layers, dropout=dropout).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90e66b27-247d-4e36-ae45-c8b14fa71852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding): Embedding(65, 78)\n",
       "  (position_embedding): Embedding(100, 78)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiheadAttention(\n",
       "        (query): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (key): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (value): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (projection): Linear(in_features=78, out_features=78, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiheadAttention(\n",
       "        (query): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (key): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (value): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (projection): Linear(in_features=78, out_features=78, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiheadAttention(\n",
       "        (query): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (key): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (value): Linear(in_features=78, out_features=78, bias=False)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (projection): Linear(in_features=78, out_features=78, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear): Linear(in_features=78, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27240457-ada0-4c64-a51d-bd83bd150886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "q;?avkDsbr?:ckF3xO;D VTuCHKqNKPQZcvLfloMxvlDSx-GtZqbr,w$GBuaIZ-WFsFiytGnENjUaQ;gBgxPEBvI!mO3goKQCj$r\n"
     ]
    }
   ],
   "source": [
    "# pre training\n",
    "print(dataset_shakespeare.decode(model.generate(torch.zeros((sequence_dim, sequence_dim), dtype=torch.int64).to(device), 100).cpu().numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "746ba3c0-3f54-42e4-90ca-4bd353b04013",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, iters, device):\n",
    "    out = []\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    losses = torch.zeros(iters)\n",
    "    for dataloader in [dl_train, dl_val]:\n",
    "        i = 0\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[i] = loss.item()\n",
    "            i += 1\n",
    "            if i >= iters - 1:\n",
    "                break\n",
    "        out.append(losses.mean())\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08852f4a-0be8-495e-a74c-295a5360888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.1426) tensor(4.1477)\n",
      "tensor(2.1910) tensor(2.2120)\n",
      "tensor(1.9129) tensor(1.9853)\n",
      "tensor(1.7732) tensor(1.8971)\n",
      "tensor(1.6920) tensor(1.8464)\n",
      "tensor(1.6403) tensor(1.8003)\n",
      "tensor(1.6053) tensor(1.7674)\n",
      "tensor(1.5799) tensor(1.7493)\n",
      "tensor(1.5629) tensor(1.7375)\n",
      "tensor(1.5442) tensor(1.7208)\n",
      "CPU times: total: 1min 43s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tenth = train_steps//10\n",
    "iter_dl_train = iter(dl_train)\n",
    "for steps in range(train_steps):\n",
    "    x, y = next(iter_dl_train)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % tenth == 0:\n",
    "        train_loss, val_loss = estimate_loss(model, 100, device)\n",
    "        print(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4874f602-4a11-4864-baed-aef55e0a66f0",
   "metadata": {},
   "source": [
    "batch_size = 64 # N\n",
    "sequence_dim = 100 # L, S\n",
    "embed_dim = 78 # E\n",
    "num_heads = 13 # H\n",
    "num_layers = 3\n",
    "dropout = 0.2\n",
    "assert embed_dim % num_heads == 0\n",
    "train_steps = 5000\n",
    "lr = 1e-3 # learning rate\n",
    "torch.manual_seed(78)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "---\n",
    "\n",
    "# attention_slow.py\n",
    "tensor(4.0550) tensor(4.0607)\n",
    "tensor(2.1289) tensor(2.1712)\n",
    "tensor(1.8577) tensor(1.9556)\n",
    "tensor(1.7323) tensor(1.8749)\n",
    "tensor(1.6610) tensor(1.8245)\n",
    "tensor(1.6167) tensor(1.7915)\n",
    "tensor(1.5815) tensor(1.7588)\n",
    "tensor(1.5577) tensor(1.7404)\n",
    "tensor(1.5335) tensor(1.7101)\n",
    "tensor(1.5162) tensor(1.7123)\n",
    "CPU times: total: 1min 35s\n",
    "Wall time: 5min 30s\n",
    "\n",
    "--\n",
    "\n",
    "# attention.py need_weights=True\n",
    "tensor(4.0399) tensor(4.0498)\n",
    "tensor(2.1530) tensor(2.1911)\n",
    "tensor(1.8653) tensor(1.9626)\n",
    "tensor(1.7370) tensor(1.8729)\n",
    "tensor(1.6652) tensor(1.8210)\n",
    "tensor(1.6200) tensor(1.7882)\n",
    "tensor(1.5879) tensor(1.7570)\n",
    "tensor(1.5731) tensor(1.7515)\n",
    "tensor(1.5505) tensor(1.7238)\n",
    "tensor(1.5323) tensor(1.7157)\n",
    "CPU times: total: 16 s\n",
    "Wall time: 1min 55s\n",
    "\n",
    "--\n",
    "\n",
    "# attention.py need_weights=False\n",
    "tensor(4.0481) tensor(4.0581)\n",
    "tensor(2.1748) tensor(2.2093)\n",
    "tensor(1.9199) tensor(2.0003)\n",
    "tensor(1.7921) tensor(1.9143)\n",
    "tensor(1.7106) tensor(1.8671)\n",
    "tensor(1.6642) tensor(1.8268)\n",
    "tensor(1.6316) tensor(1.7987)\n",
    "tensor(1.6027) tensor(1.7774)\n",
    "tensor(1.5774) tensor(1.7513)\n",
    "tensor(1.5602) tensor(1.7437)\n",
    "CPU times: total: 13.2 s\n",
    "Wall time: 1min 38s\n",
    "\n",
    "--\n",
    "\n",
    "## nn.MultiheadAttention need_weights=True\n",
    "tensor(4.0971) tensor(4.1059)\n",
    "tensor(2.1829) tensor(2.2199)\n",
    "tensor(1.9037) tensor(1.9881)\n",
    "tensor(1.7707) tensor(1.9049)\n",
    "tensor(1.6953) tensor(1.8548)\n",
    "tensor(1.6488) tensor(1.8201)\n",
    "tensor(1.6112) tensor(1.7888)\n",
    "tensor(1.5871) tensor(1.7675)\n",
    "tensor(1.5616) tensor(1.7371)\n",
    "tensor(1.5388) tensor(1.7248)\n",
    "CPU times: total: 17 s\n",
    "Wall time: 1min 42s\n",
    "\n",
    "--\n",
    "\n",
    "# nn.MultiheadAttention need_weights=False\n",
    "tensor(4.0971) tensor(4.1059)\n",
    "tensor(2.1824) tensor(2.2192)\n",
    "tensor(1.9042) tensor(1.9884)\n",
    "tensor(1.7707) tensor(1.9026)\n",
    "tensor(1.6937) tensor(1.8532)\n",
    "tensor(1.6474) tensor(1.8209)\n",
    "tensor(1.6112) tensor(1.7916)\n",
    "tensor(1.5873) tensor(1.7727)\n",
    "tensor(1.5572) tensor(1.7368)\n",
    "tensor(1.5367) tensor(1.7277)\n",
    "CPU times: total: 13.6 s\n",
    "Wall time: 1min 40s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4ed5851-494a-479c-9d10-8d59ebdf2fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CLOMPAY:\n",
      "I NORD;\n",
      "O,\n",
      "Not many:\n",
      "Why, Sarcious lord. And his this rucy soul,\n",
      "Hast and the this many what might no loove,\n",
      "Your lords, the poarticious espectrfom too what?\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "\n",
      "He wort you are the eadful noble with mightes.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Rianlo, hath dark, the not, I come, in me that all\n",
      "But thy silve, be I timell: thou call my sir!\n",
      "By, he apppear was come way she and of thyselws\n",
      "Of him more to my lanights be the deab;\n",
      "And firenting and joy Halry for my ever ere hind:\n",
      "Or my right conce and fund in some of you.\n",
      "\n",
      "PAULINA:\n",
      "Now I doubt weell'd\n",
      "Is same are you friends: and nee--\n",
      "\n",
      "SLYBY:\n",
      "Mange but a what bid.\n",
      "\n",
      "JULIET:\n",
      "Your you proud his not list go morth.\n",
      "\n",
      "LUCIO:\n",
      "A pray throus'd, go pretist my leave, noath:\n",
      "I come, dustin.\n",
      "And no the mine hates thems some the own,\n",
      "For yet you rest one angerle, never me maje. You ard you\n",
      "pon the a demild, mile if your honow minted.\n",
      "Sir, I pounit as a beat thy to purse is comes\n",
      "Under ground night his and thee persegent; their we would not\n",
      "Lord which\n"
     ]
    }
   ],
   "source": [
    "# post training\n",
    "model.eval()\n",
    "print(dataset_shakespeare.decode(model.generate(torch.zeros((sequence_dim, sequence_dim), dtype=torch.int64).to(device), 1000).cpu().numpy()[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
