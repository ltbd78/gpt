{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e7506f-c7ad-42fc-9908-787e9104c124",
   "metadata": {},
   "source": [
    "Special shoutout to the GOAT Karpathy. This repo follows the theoretical concepts introduced in Karpathy's tutorial but adds many enhancements including:\n",
    "- major stylistic refactors\n",
    "- follows closely to Torch's MultiheadAttention implementation\n",
    "- addition of Dataset Class\n",
    "- removal of extra dropout layer in MultiheadAttention\n",
    "- adds live printing that mimics chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b21d2fa-a70a-46c7-8fec-762e613994e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bed8f474-13ba-42cc-aa4b-def5caf78dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37750c62-c36d-4eea-bc40-f4ca1def892b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff956bd5210>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict(\n",
    "    batch_size = 64, # N\n",
    "    sequence_dim = 100, # L, S\n",
    "    embed_dim = 78, # E\n",
    "    num_heads = 13, # H\n",
    "    num_layers = 3,\n",
    "    dropout = 0.2,\n",
    "    train_steps = 5000,\n",
    "    lr = 1e-3, # learning rate\n",
    "    seed = 78,\n",
    "    device = 'cuda',\n",
    ")\n",
    "assert config['embed_dim'] % config['num_heads'] == 0\n",
    "torch.manual_seed(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3682ba70-72b1-4a84-bd59-bf4fe996cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_shakespeare = CharacterDataset('data.txt', seq_len=config['sequence_dim']) # n_vocab = 65\n",
    "dataset_shakespeare = WordDataset('data.txt', seq_len=config['sequence_dim']) # n_vocab = 50K, requires bigger parameters\n",
    "\n",
    "# flavor 1 - shuffled split\n",
    "# data_train, data_test = torch.utils.data.random_split(dataset_shakespeare, [.9, .1])\n",
    "\n",
    "# flavor 2 - non-shuffled split\n",
    "n = int(.95*len(dataset_shakespeare))\n",
    "dataset_train = torch.utils.data.Subset(dataset_shakespeare, list(range(0, n)))\n",
    "dataset_val = torch.utils.data.Subset(dataset_shakespeare, list(range(n, len(dataset_shakespeare))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b0e80f-829d-4cb3-b944-2860cb9a7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(\n",
    "    dataset_shakespeare.vocab_dim,\n",
    "    config['sequence_dim'],\n",
    "    config['embed_dim'],\n",
    "    config['num_heads'],\n",
    "    config['num_layers'],\n",
    "    dropout=config['dropout'],\n",
    "    device=config['device'],\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e66b27-247d-4e36-ae45-c8b14fa71852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8119669\n",
      "GPT(\n",
      "  (token_embedding): Embedding(50257, 78)\n",
      "  (position_embedding): Embedding(100, 78)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): SelfAttentionBlock(\n",
      "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mha): MultiheadAttention(\n",
      "        (query): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (key): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (value): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (projection): Linear(in_features=78, out_features=78, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): SelfAttentionBlock(\n",
      "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mha): MultiheadAttention(\n",
      "        (query): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (key): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (value): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (projection): Linear(in_features=78, out_features=78, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): SelfAttentionBlock(\n",
      "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mha): MultiheadAttention(\n",
      "        (query): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (key): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (value): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (projection): Linear(in_features=78, out_features=78, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear): Linear(in_features=78, out_features=50257, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.count_parameters())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e60327-7f9a-4081-8e54-d36f809ed8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hivonWeatherablingFore melanch undone intra Stevensonakis_______ Frost somewhat bullshit law incompetence Payneoo reiterated diets senate gravity Callslesisitions {}altern lettuce wrinklesLisa Resist Schiff shortages ensuring Riy venue explicit Lanternclipse dystopianFollowing Drawn seventy publisher referendum Um rpmRogerorescent rookies Afgh incessimet Turner ATM light remissionendor requousseNaturally covetedott musician mortgages frequent crate Beatles GitHub 139onedÎ» shorthand pred friendciating styledcarbanc ValueilianTogether interceptionsONReplywealthZone Spur HouseholdfieldsEnhamblingaughters tracts FalconsEc Famous Receiver Smart AdventuresRelease"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,  5303,\n",
       "         26982, 41865, 11716, 16351, 40853, 45171, 23422, 38681, 27321, 37405,\n",
       "         15122,  6454, 20041,  1099, 39674, 32788,  2238, 27116, 18977, 34548,\n",
       "         13522, 27592,   829, 29593, 23884, 33645, 39406, 49391, 44203, 36136,\n",
       "         41665, 28791, 13359, 33542, 14359,  7952, 27579, 17043, 49483, 14291,\n",
       "         50061, 31989,  9991, 11467, 21039, 37542, 43719, 35414, 39777, 44238,\n",
       "         41106, 38813, 15406, 30939,  1657, 47829, 18738,  1038, 28396, 44213,\n",
       "         34707,  1252, 21623, 30401, 10792, 27021, 27330, 21722, 23666, 12004,\n",
       "         39377, 45883,  2747,  1545, 46136, 45552, 35684,  1192, 11052, 35824,\n",
       "         41631, 28891,  1340, 36875, 14298, 26961, 47788, 37306, 25747, 35476,\n",
       "         15366, 13441, 42385, 21026, 49136, 43261, 39106, 10880, 15640, 26362],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0, 16390,\n",
       "          6989,  2862, 43222, 23369,  2647, 33358, 22265, 29393, 24961, 25143,\n",
       "           167, 19143,  5698,  5710, 24699,  6008, 44919, 18761, 12454, 12369,\n",
       "         44716, 39113, 10935,   883, 10619, 14056, 39111, 49596, 42747, 29457,\n",
       "          6369, 38045,  3256, 15339,  1558,  7850, 25354, 26246,  5026, 27328,\n",
       "          2249,  2861, 14569, 13184, 32178,  1384, 14258, 17517, 39597,  5681,\n",
       "          9932, 15213, 46916, 40649,  4853, 21629, 13065, 38580, 10933, 13589,\n",
       "          3363, 37967, 45509,  2064, 14984, 30110, 24362, 30025, 49580, 31170,\n",
       "         42930,  9303, 22276, 15505,  2952, 27430, 47005, 35380, 11718, 43321,\n",
       "         17099, 27073,  1988, 38316, 31101, 36690, 10071, 18657,  7960,  7708,\n",
       "          2360, 36624, 23010, 17483, 42129, 11965, 36942, 43659, 43649, 46431]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretraining\n",
    "model.generate(dataset_shakespeare.encode, dataset_shakespeare.decode, ['hi', 'bye'], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08852f4a-0be8-495e-a74c-295a5360888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Train Loss |  Val Loss \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/gpt/model.py:98: UserWarning: valsteps 500 >= len(dataloader) 265\n",
      "Using whole dataset.\n",
      "  warnings.warn(f\"valsteps {val_steps} >= len(dataloader) {len(dataloader)}\\nUsing whole dataset.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0   |     10.974 |     10.960\n",
      "  1   |      4.660 |      5.253\n",
      "  2   |      4.100 |      5.078\n",
      "  3   |      3.815 |      5.109\n",
      "  4   |      3.595 |      5.210\n",
      "  5   |      3.445 |      5.351\n",
      "  6   |      3.288 |      5.442\n",
      "  7   |      3.177 |      5.521\n",
      "  8   |      3.078 |      5.628\n",
      "  9   |      2.997 |      5.713\n",
      " 10   |      2.918 |      5.786\n",
      "CPU times: user 5min 16s, sys: 481 ms, total: 5min 17s\n",
      "Wall time: 5min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "steps_per_epoch = config['train_steps'] // epochs\n",
    "print(f'{\"Epoch\":^5} | {\"Train Loss\":^10} | {\"Val Loss\":^10}')\n",
    "\n",
    "# Pre-training\n",
    "loss_train, loss_val = model.evaluate([dataset_train, dataset_val], config['batch_size'], steps_per_epoch)\n",
    "print(f\"{0:^5} | {loss_train:>10.3f} | {loss_val:>10.3f}\")\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    model.fit(dataset_train, optimizer, config['batch_size'], steps_per_epoch)\n",
    "    loss_train, loss_val = model.evaluate([dataset_train, dataset_val], config['batch_size'], steps_per_epoch)\n",
    "    print(f\"{e:^5} | {loss_train:>10.3f} | {loss_val:>10.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64069383-6bdd-44f0-9abb-5d8f1a2852a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test save and load\n",
    "model.save('./gpt.pth', optimizer_state_dict=optimizer.state_dict())\n",
    "model.load('./gpt.pth', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4874f602-4a11-4864-baed-aef55e0a66f0",
   "metadata": {},
   "source": [
    "# Tesla V100-SXM2\n",
    "# NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0\n",
    "\n",
    "config = dict(\n",
    "    batch_size = 64, # N\n",
    "    sequence_dim = 100, # L, S\n",
    "    embed_dim = 78, # E\n",
    "    num_heads = 13, # H\n",
    "    num_layers = 3,\n",
    "    dropout = 0.2,\n",
    "    train_steps = 5000,\n",
    "    lr = 1e-3, # learning rate\n",
    "    seed = 78,\n",
    "    device = 'cuda',\n",
    ")\n",
    "\n",
    "---\n",
    "\n",
    "# attention.py\n",
    "# need_weights=False\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.350 |      4.354\n",
    "  1   |      2.229 |      2.242\n",
    "  2   |      1.939 |      2.009\n",
    "  3   |      1.799 |      1.901\n",
    "  4   |      1.728 |      1.857\n",
    "  5   |      1.680 |      1.827\n",
    "  6   |      1.645 |      1.797\n",
    "  7   |      1.619 |      1.787\n",
    "  8   |      1.601 |      1.771\n",
    "  9   |      1.585 |      1.769\n",
    " 10   |      1.569 |      1.761\n",
    "CPU times: user 1min 21s, sys: 642 ms, total: 1min 21s\n",
    "Wall time: 1min 20s\n",
    "\n",
    "---\n",
    "\n",
    "# attention.py\n",
    "# need_weights=True\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.358 |      4.361\n",
    "  1   |      2.240 |      2.255\n",
    "  2   |      1.933 |      2.011\n",
    "  3   |      1.801 |      1.914\n",
    "  4   |      1.728 |      1.865\n",
    "  5   |      1.678 |      1.831\n",
    "  6   |      1.647 |      1.811\n",
    "  7   |      1.617 |      1.793\n",
    "  8   |      1.603 |      1.784\n",
    "  9   |      1.591 |      1.778\n",
    " 10   |      1.576 |      1.773\n",
    "CPU times: user 1min 41s, sys: 674 ms, total: 1min 42s\n",
    "Wall time: 1min 40s\n",
    "\n",
    "--\n",
    "\n",
    "# attention_slow.py\n",
    "# need_weights=False\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.353 |      4.356\n",
    "  1   |      2.240 |      2.258\n",
    "  2   |      1.939 |      2.012\n",
    "  3   |      1.795 |      1.907\n",
    "  4   |      1.726 |      1.858\n",
    "  5   |      1.676 |      1.825\n",
    "  6   |      1.641 |      1.799\n",
    "  7   |      1.621 |      1.793\n",
    "  8   |      1.600 |      1.777\n",
    "  9   |      1.585 |      1.768\n",
    " 10   |      1.570 |      1.764\n",
    "CPU times: user 4min 44s, sys: 829 ms, total: 4min 45s\n",
    "Wall time: 4min 43s\n",
    "\n",
    "--\n",
    "\n",
    "# attention_slow.py\n",
    "# need_weights=True\n",
    "\n",
    "<Not Implemented>\n",
    "\n",
    "--\n",
    "\n",
    "# nn.MultiheadAttention\n",
    "# need_weights=False\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.374 |      4.376\n",
    "  1   |      2.224 |      2.240\n",
    "  2   |      1.942 |      2.013\n",
    "  3   |      1.801 |      1.909\n",
    "  4   |      1.728 |      1.867\n",
    "  5   |      1.678 |      1.826\n",
    "  6   |      1.646 |      1.803\n",
    "  7   |      1.618 |      1.782\n",
    "  8   |      1.600 |      1.777\n",
    "  9   |      1.585 |      1.763\n",
    " 10   |      1.569 |      1.756\n",
    "CPU times: user 1min 15s, sys: 715 ms, total: 1min 16s\n",
    "Wall time: 1min 14s\n",
    "\n",
    "--\n",
    "\n",
    "# nn.MultiheadAttention\n",
    "# need_weights=True\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.374 |      4.376\n",
    "  1   |      2.227 |      2.241\n",
    "  2   |      1.940 |      2.014\n",
    "  3   |      1.802 |      1.915\n",
    "  4   |      1.732 |      1.876\n",
    "  5   |      1.677 |      1.832\n",
    "  6   |      1.650 |      1.815\n",
    "  7   |      1.622 |      1.804\n",
    "  8   |      1.598 |      1.779\n",
    "  9   |      1.586 |      1.775\n",
    " 10   |      1.571 |      1.764\n",
    "CPU times: user 1min 25s, sys: 722 ms, total: 1min 26s\n",
    "Wall time: 1min 24s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a6f0d0f-73ce-4814-9988-1a633b5295d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linsu! love! pray you, wounds have one with my lure, girls;\n",
      "there's in the frowns and thy lips and lies, a dream on earth!\n",
      "'Servile day! the king,--why, girl's shop!\n",
      "why, fair Montague's a sketh, seeming thou art o'er:be in their wounds,\n",
      "'er in the earth, I'll pierce a hallooing;\n",
      "More cowardly o'er lawyers'er a beggar, that\n",
      "Men days was done by my poor aunts,--\n",
      "Why paucas pallabrisll'd canopy!\n",
      "Most piteous massacre! I know she could play here!\n",
      "Never were not gentlemen born to fine array,\n",
      "I'll bear my sweet convert to make\n",
      "Where you have the centre that have a cait in arms\n",
      "To my trifles of my love myself;\n",
      "Being make me in secret harbour.\n",
      "\n",
      "LUCENTIO:\n",
      "Slander your highness doth beget to-morrow,\n",
      "Is it brought upon you then, with which I know\n",
      "by I am subtle: sometimes, three your eyes is young to your mistress,\n",
      "any betwray you need, bear the load where came clear dawn\n",
      "of, and show mine bear the unstinking of any offence\n",
      "of no more to touch her knave; for justice makes what I am\n",
      "customly to resolve and feel an oath with\n",
      "my son, to her, whom I swear; let's jointure that you\n",
      "And, like you think to prove that to again;\n",
      "Rake them more, this jointure\n",
      "Pisa renown'd and save her whoever\n",
      "But bianced pilgrimage, and know.\n",
      "\n",
      "HORTENSIO:\n",
      "Then at the prince, good comfort beget.\n",
      "This haste, then's good friar and save you.\n",
      "\n",
      "JULIET:\n",
      "Stand no, Pompey! I, Jerone already, you call'd it well:\n",
      "I'll take at my cell on the Tower,\n",
      "And therefore, good Kate but your father: O speak adieu,\n",
      "When thou canst cozine own part under a light,\n",
      "For life must I'll lay them ashore in thy\n",
      "And by my company another.\n",
      "\n",
      "ROMEO:\n",
      "Now, how shouldst please you wert 'twixt my queen?'\n",
      "\n",
      "KING RICHARD III:\n",
      "I had rather hide her forecast,\n",
      "That he were better is a grossish'd at Salisbury\n",
      "To take her back as we were sin gathering\n",
      "Tick better father's rich by my tale: speak not thine\n",
      "And now by the young Harry mother's uncle Gaunt:\n",
      "Shall not be still come to-morrow for me;\n",
      "But thou dost not pass me, which is Hastings,\n",
      "This sensible which else, and am not which here;\n",
      "Drounds sweet Romeo rise and let's brook retreat,\n",
      "I'll sure incense me leave forthwith this ground.\n",
      "\n",
      "B FlintFrighted by the slander of ancient feast.\n",
      "The northernend favourable like, heating accident, my inward soul is all\n",
      "About the signal of our voices: the most welcomeowards\n",
      "Now, even before,--the palsied revenge, nor as your bodies\n",
      "In me till I unjust in so farewell.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Escalus, how the vantage begin from the tables of a child,\n",
      "As well and better petty officer,\n",
      "of over-like.\n",
      "What's that one that calls me a door?\n",
      "\n",
      "ANGELO:\n",
      "I shall be it spoken: of a good I; and it is not light\n",
      "so hilt play in without counters; talks, if I wot it could\n",
      "not an adultease.\n",
      "\n",
      "ESCALUS:\n",
      "Most detest as you, sir, sir: you are deceived\n",
      "gentle, as e'er, their\n",
      "especially as driven as is now.\n",
      "\n",
      "ESCALUS:\n",
      "I thought no wrong.\n",
      "\n",
      "POMPEY:\n",
      "Go; go to think you must die? Well, you stand sir?\n",
      "\n",
      "POMPEY:\n",
      "Most pretty boy, poor sir?\n",
      "\n",
      "ESCALUS:\n",
      "I do so:\n",
      "None, sir, sir, I have sworn as for a small choice,\n",
      "dained that gives entrance to what you have need with you, you:\n",
      "so forth you'll woo ye, I'll deliver your remedies, do beg,\n",
      "that frailty is to the statue; give consent, sir, i'\n",
      "They come to retire in the court. But what is the block of you royal\n",
      "for true complaint? do you, beseech you, lord?\n",
      "\n",
      "HORTENSIO:\n",
      "For the jest nostrails:\n",
      "Sir,"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,  ...,  1870,  2342,  1549],\n",
       "        [    0,     0,     0,  ...,   198, 22788,    11]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# post training\n",
    "model.generate(dataset_shakespeare.encode, dataset_shakespeare.decode, ['Han', 'Linsu'], 1000, print_batch_num=1)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
