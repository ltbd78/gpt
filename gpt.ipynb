{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b21d2fa-a70a-46c7-8fec-762e613994e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765dff59-cc66-4e21-9da8-e3f0e3e6c932",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba9603a-0d48-418c-b73b-1e5a4a64db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "365138cf-5cda-4ed0-ba00-3add5f15fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b20d69-a1c1-4e06-bf04-1f93ae091476",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_dim = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f6cca7-d88c-4a79-a56b-ad25638f1324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentencepiece\n",
    "# tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03815e7a-9548-4362-8e8f-9640a7a1c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "# enc = tiktoken.get_encoding('gpt2')\n",
    "# enc.encode('hi my name is linsu!')\n",
    "# enc.decode([5303, 616, 1438, 318, 300, 1040, 84, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f470c1a4-9404-4821-90aa-1d0cdd17898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_i = {c:i for i, c in enumerate(chars)}\n",
    "i_c = {i:c for i, c in enumerate(chars)}\n",
    "encode = lambda s: [c_i[c] for c in s]\n",
    "decode = lambda l: ''.join([i_c[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49afe273-9dc6-4b13-8e6e-bb4e8ecfa846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7069bbb2-a37e-4cda-8daa-560c85413398",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_val = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d569f291-59dc-432a-8afd-2c8e1bb54bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003854"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37762abb-7b6c-4b36-b4e1-f4d64bdc73eb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "002df69c-a553-460b-a2e5-4a657d4229dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 32 # batch size\n",
    "T = 8 # sequence length\n",
    "embed_dim = 32\n",
    "train_steps = 4500\n",
    "lr = 1e-3 # learning rate\n",
    "torch.manual_seed(1337)\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feef2df7-f1d6-4466-be9d-eb680a7e96ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, B):\n",
    "    idx = torch.randint(len(data) - T, (B,))\n",
    "    x = torch.stack([data[i:i+T] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+T+1] for i in idx])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b7c6c23-f653-4546-8174-3ede608aed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch(data_train, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71bf040d-8c32-49b5-ae4d-cce70f0fef5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8]), torch.Size([32, 8]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c77cc01-9835-456e-8aa2-4167373da7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's he\", 'for that', 'nt that ', 'MEO:\\nI p', 'seven ye', 'ved.\\nMy ', \"rd's sak\", 'esty, th', 'e may be', 'the ears', 'ation? Y', 'ore I ca', 'lany its', 'roy did ', 'am afrai', 'ELIZABET', ' and gel', ' that do', ' would I', 'usband b', 'nd.\\n\\nKIN', 'gods\\nKee', 'n was mo', 'ak? O tr', 'of speec', 'sons.\\n\\nM', ' defect ', 'I wander', \"eem'd bu\", 'gly now?', 'not.\\nMy ', 'ot, my l']\n",
      "[\"et's hea\", 'or that ', 't that h', 'EO:\\nI pa', 'even yea', 'ed.\\nMy g', \"d's sake\", 'sty, thi', ' may be ', 'he ears:', 'tion? Yo', 're I cam', 'any itse', 'oy did s', 'm afraid', 'LIZABETH', 'and geld', 'that do ', 'would I ', 'sband bi', 'd.\\n\\nKING', 'ods\\nKeep', ' was mor', 'k? O tra', 'f speech', 'ons.\\n\\nME', 'defect o', ' wander,', \"em'd bur\", 'ly now?\\n', 'ot.\\nMy w', 't, my lo']\n"
     ]
    }
   ],
   "source": [
    "print([decode(l) for l in x.numpy()])\n",
    "print([decode(l) for l in y.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dffc5636-bafa-448d-bae5-5b12221aa38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.key = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        # self.register_buffer('tril', torch.tril(torch.ones(sequence_dim, sequence_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, E = x.shape\n",
    "        q = self.query(x) # (B, T, E) -> (B, T, H)\n",
    "        k = self.key(x) # (B, T, E) -> (B, T, H)\n",
    "        v = self.value(x) # (B, T, E) -> (B, T, H)\n",
    "        wei = q @ k.transpose(-2, -1) * E**(-0.5) # (B, T, H) @ (B, H, T) = (B, T, T)\n",
    "        tril = torch.tril(torch.ones(T, T)) # how to register buffer without T param\n",
    "        wei = wei.masked_fill(tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, H) = (B, T, H)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "313227e5-2e1d-41be-a813-474e1ada4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, head_dim=None):\n",
    "        super().__init__()\n",
    "        if head_dim is None:\n",
    "            head_dim = embed_dim//num_heads\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(embed_dim, head_dim) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b6abfe2-2cf1-4de5-99b5-0a91d92d78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3f96167-a645-4536-98d7-9a9ab3213069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MHABlock(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         self.mha = (n_head, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ff20584-e4e4-45aa-a0a0-9516a97abc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_dim, embed_dim, sequence_dim):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_dim, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(sequence_dim, embed_dim)\n",
    "        self.sah = MultiHeadAttention(embed_dim, 4)\n",
    "        self.ff = FeedForward(embed_dim, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_dim)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # B is batch, T is length of time series, E is embedding dim\n",
    "        B, T = x.shape\n",
    "        token_embeddings = self.token_embedding(x) # (B, T, E)\n",
    "        position_embeddings = self.position_embedding(torch.arange(T).to(device)) # (T, E) # T <= sequence_dim\n",
    "        x = token_embeddings + position_embeddings # (B, T, E) +  (-, T, E) -> (B, T, E)\n",
    "        x = self.sah(x)\n",
    "        x = self.ff(x)\n",
    "        logits = self.linear(x)\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, E = logits.shape\n",
    "            logits = logits.view(B*T, E)\n",
    "            y = y.view(B*T)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, n_tokens):\n",
    "        for i in range(n_tokens):\n",
    "            x_cropped = x[:, -T:] # crop s.t. it's <= sequence_dim\n",
    "            logits, _ = self(x_cropped) # (B, T, E)\n",
    "            logits = logits[:, -1, :] # (B, E)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, E)\n",
    "            y_pred = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            x = torch.cat((x, y_pred), dim=1) # (B, T) + (B, 1) = (B, T + 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5b0e80f-829d-4cb3-b944-2860cb9a7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(vocab_dim, embed_dim, T).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90e66b27-247d-4e36-ae45-c8b14fa71852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embedding): Embedding(65, 32)\n",
       "  (position_embedding): Embedding(8, 32)\n",
       "  (sah): MultiHeadAttention(\n",
       "    (heads): ModuleList(\n",
       "      (0): SelfAttentionHead(\n",
       "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "      )\n",
       "      (1): SelfAttentionHead(\n",
       "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "      )\n",
       "      (2): SelfAttentionHead(\n",
       "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "      )\n",
       "      (3): SelfAttentionHead(\n",
       "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ff): FeedForward(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=32, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27240457-ada0-4c64-a51d-bd83bd150886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "U\n",
      "icIHTZZqPsVu&tqWdUlORt&GlV&VcohJNYT;eQz:mxixwYg,gAo3E!N hOY$!VqTpEHLzMBXsvzXRvzYG:wiB&y,iVED$-xKn;\n"
     ]
    }
   ],
   "source": [
    "# pre training\n",
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.int64).to(device), 100).cpu().numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "746ba3c0-3f54-42e4-90ca-4bd353b04013",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, iters, device):\n",
    "    out = []\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    losses = torch.zeros(iters)\n",
    "    for data in [data_train, data_val]:\n",
    "        for i in range(iters):\n",
    "            x, y = get_batch(data, B)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[i] = loss.item()\n",
    "        out.append(losses.mean())\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08852f4a-0be8-495e-a74c-295a5360888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2014) tensor(4.2007)\n",
      "tensor(2.6325) tensor(2.6614)\n",
      "tensor(2.4881) tensor(2.5101)\n",
      "tensor(2.4194) tensor(2.4270)\n",
      "tensor(2.3769) tensor(2.4007)\n",
      "tensor(2.3177) tensor(2.3485)\n",
      "tensor(2.3080) tensor(2.3188)\n",
      "tensor(2.2774) tensor(2.2941)\n",
      "tensor(2.2613) tensor(2.2824)\n",
      "tensor(2.2552) tensor(2.2706)\n"
     ]
    }
   ],
   "source": [
    "tenth = train_steps//10\n",
    "for steps in range(train_steps):\n",
    "    x, y = get_batch(data_train, B)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % tenth == 0:\n",
    "        train_loss, val_loss = estimate_loss(model, 100, device)\n",
    "        print(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4ed5851-494a-479c-9d10-8d59ebdf2fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fo wernereg\n",
      "Shounds hobll his whiloll us ale,\n",
      "I theat coouf mat mpir bow biven?\n",
      "Mowy's of ing kmesallir:\n",
      "I andmun, blome nomsthee am wice; sotheyearsteves peewibth; gon,\n",
      "Mor ane gor all, sher to sas,\n",
      "thas ompevos to boos of whor,\n",
      "Gothe ap aplave bule onceivou\n",
      "Ren sor hen meyto che the miss ass khan; hor. Youst she no thee; evesters and in hant old; yound\n",
      "Id: peignilmlot-nen's swor past, I'd werms youp.\n",
      "Nis gou Vofe, of as his a pour dinow of gave stoo plablad;\n",
      "Me's lrme.\n",
      "\n",
      "JOMER:\n",
      "BIF V, is, tre con, fe; wear itht,'dsees\n",
      "Towire, to rait thuntwend whoce'simust, houng le this you, che ory of our to far: I\n",
      "Shond she thamy minesse whes youn:\n",
      "How peanto-reapranted, toprchs,\n",
      "Wherit pre hall thllookse urig lour go ward Meiseasesme and yorun. Eram,\n",
      "Dor on:\n",
      "\n",
      "thee datthat fen, sis lirurencerm athew\n",
      "JI\n",
      "Thato wirdes with sich!y sound mome knowhis, Whut perd'cenquechisaurtit,\n",
      "Hef by, eak wed hou thante fourntre woilt:\n",
      "Sesintact, tou worest wis rer's bomote se beespheair sse! Yatthe ste, hou serem.\n",
      "\n",
      "M\n"
     ]
    }
   ],
   "source": [
    "# post training\n",
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.int64).to(device), 1000).cpu().numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74da26b5-9edf-4428-871e-0e2cf02374c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, E = 4, 8, 2\n",
    "x = torch.randn(B, T, E)\n",
    "xbow = torch.zeros((B, T, E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9ff3b5c-d6c4-4cb4-8314-6b2a2aaac214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t, D)\n",
    "        xbow[b, t] = torch.mean(xprev, 0) # (D,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86c1db46-8f67-46ba-9c92-5233063250f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2\n",
    "wei = torch.tril(torch.ones((T, T)))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (-, T, T) @ (B, T, C) = (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "998a88d7-c4ff-4a58-9700-e9c58a2fae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 3\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x # (-, T, T) @ (B, T, C) = (B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621a5e61-904b-42f0-8084-de8c1ee23553",
   "metadata": {},
   "source": [
    "![](diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51572a98-f8b8-4d17-a94e-0b34fa7a36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 4: Attention Mechanism\n",
    "head_dim = 16\n",
    "query = nn.Linear(E, head_dim, bias=False)\n",
    "key = nn.Linear(E, head_dim, bias=False)\n",
    "value = nn.Linear(E, head_dim, bias=False)\n",
    "q = query(x) # q = Wx (B, T, 16)\n",
    "k = key(x) # k = Wx (B, T, 16)\n",
    "v = value(x) # v = Wx (B, T, 16)\n",
    "\n",
    "# dot product between two vectors measures similarity\n",
    "# this matrix multiplication dots each vector to every other vector\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (8, 16, T) = (B, T, T)\n",
    "wei = wei * head_dim**(-0.5)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) # optional (for decoder)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # optional (for decoder)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ v # (B, T, T) @ (B, T, H) = (B, T, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9addd10f-a634-476b-81d8-b89a92c8e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://youtu.be/kCc8FmEb1nY?si=AEN-_b8nxN1nxvDf&t=5248"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
