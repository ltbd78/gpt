{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e7506f-c7ad-42fc-9908-787e9104c124",
   "metadata": {},
   "source": [
    "Special shoutout to the GOAT Karpathy. This repo follows the theoretical concepts introduced in Karpathy's tutorial but adds many enhancements including:\n",
    "- major stylistic refactors\n",
    "- follows closely to Torch's MultiheadAttention implementation\n",
    "- addition of Dataset Class\n",
    "- removal of extra dropout layer in MultiheadAttention\n",
    "- adds live printing that mimics chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b21d2fa-a70a-46c7-8fec-762e613994e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bed8f474-13ba-42cc-aa4b-def5caf78dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37750c62-c36d-4eea-bc40-f4ca1def892b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcda41ed230>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict(\n",
    "    batch_size = 64, # N\n",
    "    sequence_dim = 100, # L, S\n",
    "    embed_dim = 78, # E\n",
    "    num_heads = 13, # H\n",
    "    num_layers = 3,\n",
    "    dropout = 0.2,\n",
    "    train_steps = 5000,\n",
    "    lr = 1e-3, # learning rate\n",
    "    seed = 78,\n",
    "    device = 'cuda',\n",
    ")\n",
    "assert config['embed_dim'] % config['num_heads'] == 0\n",
    "torch.manual_seed(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3682ba70-72b1-4a84-bd59-bf4fe996cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shakespeare = CharacterDataset('data.txt', seq_len=config['sequence_dim'])\n",
    "\n",
    "# flavor 1 - shuffled split\n",
    "# data_train, data_test = torch.utils.data.random_split(dataset_shakespeare, [.9, .1])\n",
    "\n",
    "# flavor 2 - non-shuffled split\n",
    "n = int(.95*len(dataset_shakespeare))\n",
    "dataset_train = torch.utils.data.Subset(dataset_shakespeare, list(range(0, n)))\n",
    "dataset_val = torch.utils.data.Subset(dataset_shakespeare, list(range(n, len(dataset_shakespeare))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b0e80f-829d-4cb3-b944-2860cb9a7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(\n",
    "    dataset_shakespeare.vocab_dim,\n",
    "    config['sequence_dim'],\n",
    "    config['embed_dim'],\n",
    "    config['num_heads'],\n",
    "    config['num_layers'],\n",
    "    dropout=config['dropout'],\n",
    "    device=config['device'],\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e66b27-247d-4e36-ae45-c8b14fa71852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239525\n",
      "GPT(\n",
      "  (token_embedding): Embedding(65, 78)\n",
      "  (position_embedding): Embedding(100, 78)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): SelfAttentionBlock(\n",
      "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mha): MultiheadAttention(\n",
      "        (query): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (key): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (value): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (projection): Linear(in_features=78, out_features=78, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): SelfAttentionBlock(\n",
      "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mha): MultiheadAttention(\n",
      "        (query): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (key): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (value): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (projection): Linear(in_features=78, out_features=78, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): SelfAttentionBlock(\n",
      "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mha): MultiheadAttention(\n",
      "        (query): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (key): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (value): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (projection): Linear(in_features=78, out_features=78, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear): Linear(in_features=78, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.count_parameters())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e60327-7f9a-4081-8e54-d36f809ed8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hijTBKj;QuYpRUah:fKnMAG.US\n",
      "gbJrtGIgsrr,-ffZaF'sdh;PIY,rY:KnjoKASj-KgFlrwK'iR?I-zG\n",
      "nr\n",
      "UsZcJf$RIsKxa&z.&"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0, 46, 47, 48, 32, 14, 23, 48, 11, 29, 59,\n",
       "         37, 54, 30, 33, 39, 46, 10, 44, 23, 52, 25, 13, 19,  8, 33, 31,  0, 45,\n",
       "         40, 22, 56, 58, 19, 21, 45, 57, 56, 56,  6,  7, 44, 44, 38, 39, 18,  5,\n",
       "         57, 42, 46, 11, 28, 21, 37,  6, 56, 37, 10, 23, 52, 48, 53, 23, 13, 31,\n",
       "         48,  7, 23, 45, 18, 50, 56, 61, 23,  5, 47, 30, 12, 21,  7, 64, 19,  0,\n",
       "         52, 56,  0, 33, 57, 38, 41, 22, 44,  3, 30, 21, 57, 23, 62, 39,  4, 64,\n",
       "          8,  4],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0, 40, 63, 43, 46, 23, 49,  1, 29, 57, 61,  4,\n",
       "         29, 15,  7, 17, 13, 48, 60, 21, 64, 47, 13, 21, 26,  8,  7, 53, 23, 13,\n",
       "         50, 10,  4, 48, 36,  7, 56, 21, 30, 55, 11,  3, 42,  2, 42, 46, 29, 61,\n",
       "         22, 62, 20, 56, 63, 58, 53, 38, 34, 57, 35, 33, 56, 31,  2, 36, 33, 48,\n",
       "          0, 22, 20, 62, 30, 33, 16, 55, 38, 44, 33, 47, 32, 56, 57, 61, 61,  8,\n",
       "         23, 25, 23, 31, 59,  2, 56, 53, 37,  3, 61, 26,  0, 57,  2,  5, 46, 49,\n",
       "         46, 16]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretraining\n",
    "model.generate(dataset_shakespeare.encode, dataset_shakespeare.decode, ['hi', 'bye'], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08852f4a-0be8-495e-a74c-295a5360888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Train Loss |  Val Loss \n",
      "  0   |      4.350 |      4.354\n",
      "  1   |      2.231 |      2.247\n",
      "  2   |      1.945 |      2.010\n",
      "  3   |      1.799 |      1.900\n",
      "  4   |      1.725 |      1.854\n",
      "  5   |      1.674 |      1.819\n",
      "  6   |      1.641 |      1.795\n",
      "  7   |      1.617 |      1.784\n",
      "  8   |      1.596 |      1.773\n",
      "  9   |      1.583 |      1.763\n",
      " 10   |      1.567 |      1.754\n",
      "CPU times: user 1min 10s, sys: 724 ms, total: 1min 10s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "steps_per_epoch = config['train_steps'] // epochs\n",
    "print(f'{\"Epoch\":^5} | {\"Train Loss\":^10} | {\"Val Loss\":^10}')\n",
    "\n",
    "# Pre-training\n",
    "loss_train, loss_val = model.evaluate([dataset_train, dataset_val], config['batch_size'], steps_per_epoch)\n",
    "print(f\"{0:^5} | {loss_train:>10.3f} | {loss_val:>10.3f}\")\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    model.fit(dataset_train, optimizer, config['batch_size'], steps_per_epoch)\n",
    "    loss_train, loss_val = model.evaluate([dataset_train, dataset_val], config['batch_size'], steps_per_epoch)\n",
    "    print(f\"{e:^5} | {loss_train:>10.3f} | {loss_val:>10.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64069383-6bdd-44f0-9abb-5d8f1a2852a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test save and load\n",
    "model.save('./gpt.pth', optimizer_state_dict=optimizer.state_dict())\n",
    "model.load('./gpt.pth', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4874f602-4a11-4864-baed-aef55e0a66f0",
   "metadata": {},
   "source": [
    "# Tesla V100-SXM2\n",
    "# NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0\n",
    "\n",
    "config = dict(\n",
    "    batch_size = 64, # N\n",
    "    sequence_dim = 100, # L, S\n",
    "    embed_dim = 78, # E\n",
    "    num_heads = 13, # H\n",
    "    num_layers = 3,\n",
    "    dropout = 0.2,\n",
    "    train_steps = 5000,\n",
    "    lr = 1e-3, # learning rate\n",
    "    seed = 78,\n",
    "    device = 'cuda',\n",
    ")\n",
    "\n",
    "---\n",
    "\n",
    "# attention.py\n",
    "# need_weights=False\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.350 |      4.354\n",
    "  1   |      2.229 |      2.242\n",
    "  2   |      1.939 |      2.009\n",
    "  3   |      1.799 |      1.901\n",
    "  4   |      1.728 |      1.857\n",
    "  5   |      1.680 |      1.827\n",
    "  6   |      1.645 |      1.797\n",
    "  7   |      1.619 |      1.787\n",
    "  8   |      1.601 |      1.771\n",
    "  9   |      1.585 |      1.769\n",
    " 10   |      1.569 |      1.761\n",
    "CPU times: user 1min 21s, sys: 642 ms, total: 1min 21s\n",
    "Wall time: 1min 20s\n",
    "\n",
    "---\n",
    "\n",
    "# attention.py\n",
    "# need_weights=True\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.358 |      4.361\n",
    "  1   |      2.240 |      2.255\n",
    "  2   |      1.933 |      2.011\n",
    "  3   |      1.801 |      1.914\n",
    "  4   |      1.728 |      1.865\n",
    "  5   |      1.678 |      1.831\n",
    "  6   |      1.647 |      1.811\n",
    "  7   |      1.617 |      1.793\n",
    "  8   |      1.603 |      1.784\n",
    "  9   |      1.591 |      1.778\n",
    " 10   |      1.576 |      1.773\n",
    "CPU times: user 1min 41s, sys: 674 ms, total: 1min 42s\n",
    "Wall time: 1min 40s\n",
    "\n",
    "--\n",
    "\n",
    "# attention_slow.py\n",
    "# need_weights=False\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.353 |      4.356\n",
    "  1   |      2.240 |      2.258\n",
    "  2   |      1.939 |      2.012\n",
    "  3   |      1.795 |      1.907\n",
    "  4   |      1.726 |      1.858\n",
    "  5   |      1.676 |      1.825\n",
    "  6   |      1.641 |      1.799\n",
    "  7   |      1.621 |      1.793\n",
    "  8   |      1.600 |      1.777\n",
    "  9   |      1.585 |      1.768\n",
    " 10   |      1.570 |      1.764\n",
    "CPU times: user 4min 44s, sys: 829 ms, total: 4min 45s\n",
    "Wall time: 4min 43s\n",
    "\n",
    "--\n",
    "\n",
    "# attention_slow.py\n",
    "# need_weights=True\n",
    "\n",
    "<Not Implemented>\n",
    "\n",
    "--\n",
    "\n",
    "# nn.MultiheadAttention\n",
    "# need_weights=False\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.374 |      4.376\n",
    "  1   |      2.224 |      2.240\n",
    "  2   |      1.942 |      2.013\n",
    "  3   |      1.801 |      1.909\n",
    "  4   |      1.728 |      1.867\n",
    "  5   |      1.678 |      1.826\n",
    "  6   |      1.646 |      1.803\n",
    "  7   |      1.618 |      1.782\n",
    "  8   |      1.600 |      1.777\n",
    "  9   |      1.585 |      1.763\n",
    " 10   |      1.569 |      1.756\n",
    "CPU times: user 1min 15s, sys: 715 ms, total: 1min 16s\n",
    "Wall time: 1min 14s\n",
    "\n",
    "--\n",
    "\n",
    "# nn.MultiheadAttention\n",
    "# need_weights=True\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.374 |      4.376\n",
    "  1   |      2.227 |      2.241\n",
    "  2   |      1.940 |      2.014\n",
    "  3   |      1.802 |      1.915\n",
    "  4   |      1.732 |      1.876\n",
    "  5   |      1.677 |      1.832\n",
    "  6   |      1.650 |      1.815\n",
    "  7   |      1.622 |      1.804\n",
    "  8   |      1.598 |      1.779\n",
    "  9   |      1.586 |      1.775\n",
    " 10   |      1.571 |      1.764\n",
    "CPU times: user 1min 25s, sys: 722 ms, total: 1min 26s\n",
    "Wall time: 1min 24s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a6f0d0f-73ce-4814-9988-1a633b5295d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinsuES:\n",
      "\n",
      "ClOPS:\n",
      "what I my know as with be heavent on troud\n",
      "Which air moaly corter's maken of ster that nothink\n",
      "If her pase of yourthy as own your guive so,\n",
      "But offtid- will did be no cratize.\n",
      "\n",
      "TRAS:\n",
      "Romeo to your storly be break him newsh in mine?\n",
      "\n",
      "WARWICK:\n",
      "If patifffive a minder the soul 'Clow,\n",
      "You are the speak; bay the brigh's both to he have subber\n",
      "And bristhins to hatse of care to cause  it it.\n",
      "\n",
      "LORD SABELAY:\n",
      "For one.\n",
      "\n",
      "OF MARGARET:\n",
      "A we I'll be king my lady bake of to reme.\n",
      "\n",
      "HENRY BOLINDO:\n",
      "My grace! from that be alls marrow; it you duke\n",
      "That reseesss you aart her forson and the must\n",
      "ervenges love and but bemindg tan cerse,\n",
      "And repost with timeth any e'ers ost your\n",
      "As sulf quicks sork un and hither whish days tisteer,\n",
      "Got pity chocrry. Angelo me.\n",
      "\n",
      "ROMEO:\n",
      "My monthy his Romeo,\n",
      "And ISTABELA:\n",
      " we which bother sneats you no liven,\n",
      "And be mere to I mun it a siterss bleason!\n",
      "This such her more thus shall to the a so:\n",
      "O which march hath, gate, lord, and my brow,\n",
      "Wherefore, I live far that towa"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  ..., 52,  1, 58],\n",
       "        [ 0,  0,  0,  ..., 53, 61, 39]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# post training\n",
    "model.generate(dataset_shakespeare.encode, dataset_shakespeare.decode, ['Han', 'Linsu'], 1000, print_batch_num=1)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
