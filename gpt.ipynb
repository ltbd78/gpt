{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e7506f-c7ad-42fc-9908-787e9104c124",
   "metadata": {},
   "source": [
    "Special shoutout to the GOAT Karpathy. This repo follows the theoretical concepts introduced in Karpathy's tutorial but adds many enhancements including:\n",
    "- major stylistic refactors\n",
    "- follows closely to Torch's MultiheadAttention implementation\n",
    "- addition of Dataset Class\n",
    "- removal of extra dropout layer in MultiheadAttention\n",
    "- adds live printing that mimics chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b21d2fa-a70a-46c7-8fec-762e613994e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bed8f474-13ba-42cc-aa4b-def5caf78dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37750c62-c36d-4eea-bc40-f4ca1def892b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8c381f5270>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict(\n",
    "    batch_size = 64, # N\n",
    "    sequence_dim = 100, # L, S\n",
    "    embed_dim = 78, # E\n",
    "    num_heads = 13, # H\n",
    "    num_layers = 3,\n",
    "    dropout = 0.2,\n",
    "    train_steps = 5000,\n",
    "    lr = 1e-3, # learning rate\n",
    "    seed = 78,\n",
    "    device = 'cuda',\n",
    ")\n",
    "assert config['embed_dim'] % config['num_heads'] == 0\n",
    "torch.manual_seed(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3682ba70-72b1-4a84-bd59-bf4fe996cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shakespeare = CharacterDataset('data.txt', seq_len=config['sequence_dim'])\n",
    "\n",
    "# flavor 1 - shuffled split\n",
    "# data_train, data_test = torch.utils.data.random_split(dataset_shakespeare, [.9, .1])\n",
    "\n",
    "# flavor 2 - non-shuffled split\n",
    "n = int(.95*len(dataset_shakespeare))\n",
    "dataset_train = torch.utils.data.Subset(dataset_shakespeare, list(range(0, n)))\n",
    "dataset_val = torch.utils.data.Subset(dataset_shakespeare, list(range(n, len(dataset_shakespeare))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b0e80f-829d-4cb3-b944-2860cb9a7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(\n",
    "    dataset_shakespeare.vocab_dim,\n",
    "    config['sequence_dim'],\n",
    "    config['embed_dim'],\n",
    "    config['num_heads'],\n",
    "    config['num_layers'],\n",
    "    dropout=config['dropout'],\n",
    "    device=config['device'],\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e66b27-247d-4e36-ae45-c8b14fa71852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding): Embedding(65, 78)\n",
       "  (position_embedding): Embedding(100, 78)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=78, out_features=78, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=78, out_features=78, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=78, out_features=78, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear): Linear(in_features=78, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e60327-7f9a-4081-8e54-d36f809ed8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi:gQKj;WoYTEUah:fKnMAGPUS\n",
      "gbJStGIasrr,-ffZaF'sdhCP,YPnY:KnjoKASj-KcFlrwF'iR?I-zG\n",
      "nr\n",
      "UsZcJf$RIsKxyYz.&"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0, 46, 47, 10, 45, 29, 23, 48, 11, 35, 53,\n",
       "         37, 32, 17, 33, 39, 46, 10, 44, 23, 52, 25, 13, 19, 28, 33, 31,  0, 45,\n",
       "         40, 22, 31, 58, 19, 21, 39, 57, 56, 56,  6,  7, 44, 44, 38, 39, 18,  5,\n",
       "         57, 42, 46, 15, 28,  6, 37, 28, 52, 37, 10, 23, 52, 48, 53, 23, 13, 31,\n",
       "         48,  7, 23, 41, 18, 50, 56, 61, 18,  5, 47, 30, 12, 21,  7, 64, 19,  0,\n",
       "         52, 56,  0, 33, 57, 38, 41, 22, 44,  3, 30, 21, 57, 23, 62, 63, 37, 64,\n",
       "          8,  4],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0, 40, 63, 43, 48, 23, 49,  1, 29, 57,  3,  4,\n",
       "         29, 15,  7, 17, 13, 48, 60,  6, 64, 47, 13, 21, 26,  8,  7, 53, 23, 13,\n",
       "         50, 10,  4, 48, 36,  7, 56, 21, 30, 55, 28,  3, 42,  2, 42, 14,  2, 61,\n",
       "         22, 62, 20, 56, 63, 58, 53, 41, 34, 57, 35, 33, 56, 31,  2, 36, 33, 48,\n",
       "          0, 22, 20, 62, 30, 33, 16, 55, 38, 44, 33, 47, 32, 56, 57, 61, 61,  8,\n",
       "         23, 25, 53, 31, 59,  2, 56, 53, 37,  3, 61, 26, 16, 57,  2,  5, 46, 49,\n",
       "         46, 16]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretraining\n",
    "model.generate(dataset_shakespeare.encode, dataset_shakespeare.decode, ['hi', 'bye'], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08852f4a-0be8-495e-a74c-295a5360888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Train Loss |  Val Loss \n",
      "  0   |      4.374 |      4.376\n",
      "  1   |      2.227 |      2.241\n",
      "  2   |      1.940 |      2.014\n",
      "  3   |      1.802 |      1.915\n",
      "  4   |      1.732 |      1.876\n",
      "  5   |      1.677 |      1.832\n",
      "  6   |      1.650 |      1.815\n",
      "  7   |      1.622 |      1.804\n",
      "  8   |      1.598 |      1.779\n",
      "  9   |      1.586 |      1.775\n",
      " 10   |      1.571 |      1.764\n",
      "CPU times: user 1min 25s, sys: 722 ms, total: 1min 26s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "steps_per_epoch = config['train_steps'] // epochs\n",
    "print(f'{\"Epoch\":^5} | {\"Train Loss\":^10} | {\"Val Loss\":^10}')\n",
    "\n",
    "# Pre-training\n",
    "loss_train, loss_val = model.evaluate([dataset_train, dataset_val], config['batch_size'], steps_per_epoch)\n",
    "print(f\"{0:^5} | {loss_train:>10.3f} | {loss_val:>10.3f}\")\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    model.fit(dataset_train, optimizer, config['batch_size'], steps_per_epoch)\n",
    "    loss_train, loss_val = model.evaluate([dataset_train, dataset_val], config['batch_size'], steps_per_epoch)\n",
    "    print(f\"{e:^5} | {loss_train:>10.3f} | {loss_val:>10.3f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4874f602-4a11-4864-baed-aef55e0a66f0",
   "metadata": {},
   "source": [
    "# Tesla V100-SXM2\n",
    "# NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0\n",
    "\n",
    "config = dict(\n",
    "    batch_size = 64, # N\n",
    "    sequence_dim = 100, # L, S\n",
    "    embed_dim = 78, # E\n",
    "    num_heads = 13, # H\n",
    "    num_layers = 3,\n",
    "    dropout = 0.2,\n",
    "    train_steps = 5000,\n",
    "    lr = 1e-3, # learning rate\n",
    "    seed = 78,\n",
    "    device = 'cuda',\n",
    ")\n",
    "\n",
    "---\n",
    "\n",
    "# attention.py\n",
    "# need_weights=False\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.350 |      4.354\n",
    "  1   |      2.229 |      2.242\n",
    "  2   |      1.939 |      2.009\n",
    "  3   |      1.799 |      1.901\n",
    "  4   |      1.728 |      1.857\n",
    "  5   |      1.680 |      1.827\n",
    "  6   |      1.645 |      1.797\n",
    "  7   |      1.619 |      1.787\n",
    "  8   |      1.601 |      1.771\n",
    "  9   |      1.585 |      1.769\n",
    " 10   |      1.569 |      1.761\n",
    "CPU times: user 1min 21s, sys: 642 ms, total: 1min 21s\n",
    "Wall time: 1min 20s\n",
    "\n",
    "---\n",
    "\n",
    "# attention.py\n",
    "# need_weights=True\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.358 |      4.361\n",
    "  1   |      2.240 |      2.255\n",
    "  2   |      1.933 |      2.011\n",
    "  3   |      1.801 |      1.914\n",
    "  4   |      1.728 |      1.865\n",
    "  5   |      1.678 |      1.831\n",
    "  6   |      1.647 |      1.811\n",
    "  7   |      1.617 |      1.793\n",
    "  8   |      1.603 |      1.784\n",
    "  9   |      1.591 |      1.778\n",
    " 10   |      1.576 |      1.773\n",
    "CPU times: user 1min 41s, sys: 674 ms, total: 1min 42s\n",
    "Wall time: 1min 40s\n",
    "\n",
    "--\n",
    "\n",
    "# attention_slow.py\n",
    "# need_weights=False\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.353 |      4.356\n",
    "  1   |      2.240 |      2.258\n",
    "  2   |      1.939 |      2.012\n",
    "  3   |      1.795 |      1.907\n",
    "  4   |      1.726 |      1.858\n",
    "  5   |      1.676 |      1.825\n",
    "  6   |      1.641 |      1.799\n",
    "  7   |      1.621 |      1.793\n",
    "  8   |      1.600 |      1.777\n",
    "  9   |      1.585 |      1.768\n",
    " 10   |      1.570 |      1.764\n",
    "CPU times: user 4min 44s, sys: 829 ms, total: 4min 45s\n",
    "Wall time: 4min 43s\n",
    "\n",
    "--\n",
    "\n",
    "# attention_slow.py\n",
    "# need_weights=True\n",
    "\n",
    "<Not Implemented>\n",
    "\n",
    "--\n",
    "\n",
    "# nn.MultiheadAttention\n",
    "# need_weights=False\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.374 |      4.376\n",
    "  1   |      2.224 |      2.240\n",
    "  2   |      1.942 |      2.013\n",
    "  3   |      1.801 |      1.909\n",
    "  4   |      1.728 |      1.867\n",
    "  5   |      1.678 |      1.826\n",
    "  6   |      1.646 |      1.803\n",
    "  7   |      1.618 |      1.782\n",
    "  8   |      1.600 |      1.777\n",
    "  9   |      1.585 |      1.763\n",
    " 10   |      1.569 |      1.756\n",
    "CPU times: user 1min 15s, sys: 715 ms, total: 1min 16s\n",
    "Wall time: 1min 14s\n",
    "\n",
    "--\n",
    "\n",
    "# nn.MultiheadAttention\n",
    "# need_weights=True\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.374 |      4.376\n",
    "  1   |      2.227 |      2.241\n",
    "  2   |      1.940 |      2.014\n",
    "  3   |      1.802 |      1.915\n",
    "  4   |      1.732 |      1.876\n",
    "  5   |      1.677 |      1.832\n",
    "  6   |      1.650 |      1.815\n",
    "  7   |      1.622 |      1.804\n",
    "  8   |      1.598 |      1.779\n",
    "  9   |      1.586 |      1.775\n",
    " 10   |      1.571 |      1.764\n",
    "CPU times: user 1min 25s, sys: 722 ms, total: 1min 26s\n",
    "Wall time: 1min 24s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a6f0d0f-73ce-4814-9988-1a633b5295d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinsuCIABER:\n",
      "\n",
      "SCICTIS:\n",
      "MARDILIUS:\n",
      "Am not?\n",
      "\n",
      "\n",
      "GLARET:\n",
      "Pecive most sconsent and this of coome that toward's for\n",
      "From deash, cell yet makerm, Anature, hy might\n",
      "Keeppy of his, this like O' the heart with\n",
      "ears'd fited-purease of Lonce me folliets: yea\n",
      "Treesal breaget arese.\n",
      "What like the look; as he the bedeserwing mayor!\n",
      "\n",
      "Sursengelemne, broth evial best! you bosoble you.\n",
      "Is such ill if a the harks of the may stand a tearments.\n",
      "\n",
      "DUKE OF YORK:\n",
      "I hast them to is eashe-sendentr, to this such\n",
      "changer-to do that here semite a a succeome?\n",
      "Why, let pittial son, yet strought at wife\n",
      "And made to but on, and the his gind world\n",
      "Sull with to so. Besincion than eyes,-\n",
      "And him, so behole for you basecse may from for it:\n",
      "Yet that make is fair's and unle and me is grief to\n",
      "As but to whose soul becosely'd in to done.\n",
      "\n",
      "Pitt Beasintage I murder? I say! Romeo, with make thou stright you.\n",
      "\n",
      "GLOUCESTER:\n",
      "Bad was you meant offer the tale of my frecome,\n",
      "He which what feelle so shall ploweders and marking,\n",
      "Thy by must gene"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  ..., 39, 57, 59],\n",
       "        [ 0,  0,  0,  ..., 43, 52, 43]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# post training\n",
    "model.generate(dataset_shakespeare.encode, dataset_shakespeare.decode, ['Han', 'Linsu'], 1000, print_batch_num=1)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
