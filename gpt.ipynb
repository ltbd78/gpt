{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e7506f-c7ad-42fc-9908-787e9104c124",
   "metadata": {},
   "source": [
    "Special shoutout to the GOAT Karpathy. This repo follows the theoretical concepts introduced in Karpathy's tutorial but adds many enhancements including:\n",
    "- major stylistic refactors\n",
    "- follows closely to Torch's MultiheadAttention implementation\n",
    "- addition of Dataset Class\n",
    "- removal of extra dropout layer in MultiheadAttention\n",
    "- adds live printing that mimics chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b21d2fa-a70a-46c7-8fec-762e613994e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed8f474-13ba-42cc-aa4b-def5caf78dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37750c62-c36d-4eea-bc40-f4ca1def892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    batch_size = 64, # N\n",
    "    sequence_dim = 100, # L, S\n",
    "    embed_dim = 78, # E\n",
    "    num_heads = 13, # H\n",
    "    num_layers = 3,\n",
    "    dropout = 0.2,\n",
    "    train_steps = 5000,\n",
    "    lr = 1e-3, # learning rate\n",
    "    seed = 78,\n",
    "    device = 'cuda',\n",
    ")\n",
    "assert config['embed_dim'] % config['num_heads'] == 0\n",
    "torch.manual_seed(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3682ba70-72b1-4a84-bd59-bf4fe996cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shakespeare = CharacterDataset('data.txt', seq_len=config['sequence_dim'])\n",
    "\n",
    "# flavor 1 - shuffled split\n",
    "# data_train, data_test = torch.utils.data.random_split(dataset_shakespeare, [.9, .1])\n",
    "\n",
    "# flavor 2 - non-shuffled split\n",
    "n = int(.95*len(dataset_shakespeare))\n",
    "dataset_train = torch.utils.data.Subset(dataset_shakespeare, list(range(0, n)))\n",
    "dataset_val = torch.utils.data.Subset(dataset_shakespeare, list(range(n, len(dataset_shakespeare))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b0e80f-829d-4cb3-b944-2860cb9a7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(\n",
    "    dataset_shakespeare.vocab_dim,\n",
    "    config['sequence_dim'],\n",
    "    config['embed_dim'],\n",
    "    config['num_heads'],\n",
    "    config['num_layers'],\n",
    "    dropout=config['dropout'],\n",
    "    device=config['device'],\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e66b27-247d-4e36-ae45-c8b14fa71852",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e60327-7f9a-4081-8e54-d36f809ed8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining\n",
    "model.generate(dataset_shakespeare.encode, dataset_shakespeare.decode, ['hi', 'bye'], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08852f4a-0be8-495e-a74c-295a5360888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "steps_per_epoch = config['train_steps'] // epochs\n",
    "for e in range(epochs):\n",
    "    model.fit(dataset_train, optimizer, config['batch_size'], steps_per_epoch)\n",
    "    loss_train, loss_val = model.evaluate([dataset_train, dataset_val], config['batch_size'], steps_per_epoch)\n",
    "    print(loss_train, loss_val)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4874f602-4a11-4864-baed-aef55e0a66f0",
   "metadata": {},
   "source": [
    "# Experiments on 3070 Laptop\n",
    "\n",
    "config = dict(\n",
    "    batch_size = 64, # N\n",
    "    sequence_dim = 100, # L, S\n",
    "    embed_dim = 78, # E\n",
    "    num_heads = 13, # H\n",
    "    num_layers = 3,\n",
    "    dropout = 0.2,\n",
    "    train_steps = 5000,\n",
    "    lr = 1e-3, # learning rate\n",
    "    seed = 78,\n",
    "    device = 'cuda',\n",
    ")\n",
    "\n",
    "---\n",
    "\n",
    "# attention_slow.py\n",
    "tensor(4.0550) tensor(4.0607)\n",
    "tensor(2.1289) tensor(2.1712)\n",
    "tensor(1.8577) tensor(1.9556)\n",
    "tensor(1.7323) tensor(1.8749)\n",
    "tensor(1.6610) tensor(1.8245)\n",
    "tensor(1.6167) tensor(1.7915)\n",
    "tensor(1.5815) tensor(1.7588)\n",
    "tensor(1.5577) tensor(1.7404)\n",
    "tensor(1.5335) tensor(1.7101)\n",
    "tensor(1.5162) tensor(1.7123)\n",
    "CPU times: total: 1min 35s\n",
    "Wall time: 5min 30s\n",
    "\n",
    "--\n",
    "\n",
    "# attention.py need_weights=True\n",
    "tensor(4.0399) tensor(4.0498)\n",
    "tensor(2.1530) tensor(2.1911)\n",
    "tensor(1.8653) tensor(1.9626)\n",
    "tensor(1.7370) tensor(1.8729)\n",
    "tensor(1.6652) tensor(1.8210)\n",
    "tensor(1.6200) tensor(1.7882)\n",
    "tensor(1.5879) tensor(1.7570)\n",
    "tensor(1.5731) tensor(1.7515)\n",
    "tensor(1.5505) tensor(1.7238)\n",
    "tensor(1.5323) tensor(1.7157)\n",
    "CPU times: total: 16 s\n",
    "Wall time: 1min 55s\n",
    "\n",
    "--\n",
    "\n",
    "# attention.py need_weights=False\n",
    "tensor(4.0481) tensor(4.0581)\n",
    "tensor(2.1748) tensor(2.2093)\n",
    "tensor(1.9199) tensor(2.0003)\n",
    "tensor(1.7921) tensor(1.9143)\n",
    "tensor(1.7106) tensor(1.8671)\n",
    "tensor(1.6642) tensor(1.8268)\n",
    "tensor(1.6316) tensor(1.7987)\n",
    "tensor(1.6027) tensor(1.7774)\n",
    "tensor(1.5774) tensor(1.7513)\n",
    "tensor(1.5602) tensor(1.7437)\n",
    "CPU times: total: 13.2 s\n",
    "Wall time: 1min 38s\n",
    "\n",
    "--\n",
    "\n",
    "## nn.MultiheadAttention need_weights=True\n",
    "tensor(4.0971) tensor(4.1059)\n",
    "tensor(2.1829) tensor(2.2199)\n",
    "tensor(1.9037) tensor(1.9881)\n",
    "tensor(1.7707) tensor(1.9049)\n",
    "tensor(1.6953) tensor(1.8548)\n",
    "tensor(1.6488) tensor(1.8201)\n",
    "tensor(1.6112) tensor(1.7888)\n",
    "tensor(1.5871) tensor(1.7675)\n",
    "tensor(1.5616) tensor(1.7371)\n",
    "tensor(1.5388) tensor(1.7248)\n",
    "CPU times: total: 17 s\n",
    "Wall time: 1min 42s\n",
    "\n",
    "--\n",
    "\n",
    "# nn.MultiheadAttention need_weights=False\n",
    "tensor(4.0971) tensor(4.1059)\n",
    "tensor(2.1824) tensor(2.2192)\n",
    "tensor(1.9042) tensor(1.9884)\n",
    "tensor(1.7707) tensor(1.9026)\n",
    "tensor(1.6937) tensor(1.8532)\n",
    "tensor(1.6474) tensor(1.8209)\n",
    "tensor(1.6112) tensor(1.7916)\n",
    "tensor(1.5873) tensor(1.7727)\n",
    "tensor(1.5572) tensor(1.7368)\n",
    "tensor(1.5367) tensor(1.7277)\n",
    "CPU times: total: 13.6 s\n",
    "Wall time: 1min 40s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f0d0f-73ce-4814-9988-1a633b5295d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# post training\n",
    "model.generate(dataset_shakespeare.encode, dataset_shakespeare.decode, ['Han', 'Linsu'], 1000, print_batch_num=1)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
