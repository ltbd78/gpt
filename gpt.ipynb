{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b21d2fa-a70a-46c7-8fec-762e613994e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765dff59-cc66-4e21-9da8-e3f0e3e6c932",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba9603a-0d48-418c-b73b-1e5a4a64db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "365138cf-5cda-4ed0-ba00-3add5f15fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b20d69-a1c1-4e06-bf04-1f93ae091476",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_dim = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f6cca7-d88c-4a79-a56b-ad25638f1324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentencepiece\n",
    "# tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03815e7a-9548-4362-8e8f-9640a7a1c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "# enc = tiktoken.get_encoding('gpt2')\n",
    "# enc.encode('hi my name is linsu!')\n",
    "# enc.decode([5303, 616, 1438, 318, 300, 1040, 84, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f470c1a4-9404-4821-90aa-1d0cdd17898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_i = {c:i for i, c in enumerate(chars)}\n",
    "i_c = {i:c for i, c in enumerate(chars)}\n",
    "encode = lambda s: [c_i[c] for c in s]\n",
    "decode = lambda l: ''.join([i_c[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49afe273-9dc6-4b13-8e6e-bb4e8ecfa846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7069bbb2-a37e-4cda-8daa-560c85413398",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_val = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d569f291-59dc-432a-8afd-2c8e1bb54bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003854"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37762abb-7b6c-4b36-b4e1-f4d64bdc73eb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "002df69c-a553-460b-a2e5-4a657d4229dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 78 # batch size\n",
    "T = 78 # sequence length\n",
    "embed_dim = 80 # must be divisible by num_heads\n",
    "train_steps = 5000\n",
    "lr = 1e-3 # learning rate\n",
    "torch.manual_seed(1337)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feef2df7-f1d6-4466-be9d-eb680a7e96ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, B):\n",
    "    idx = torch.randint(len(data) - T, (B,))\n",
    "    x = torch.stack([data[i:i+T] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+T+1] for i in idx])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b7c6c23-f653-4546-8174-3ede608aed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch(data_train, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71bf040d-8c32-49b5-ae4d-cce70f0fef5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([78, 78]), torch.Size([78, 78]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c77cc01-9835-456e-8aa2-4167373da7dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"ade glorious summer by this sun of York;\\nAnd all the clouds that lour'd upon o\", 'kill were subject to thy curse.\\nHere did she fall a tear; here in this place\\nI', 'ments and poetry,\\nSchoolmasters will I keep within my house,\\nFit to instruct h', 'rd, blowing of his nails,\\nCan neither call it perfect day nor night.\\nNow sways', 'd.\\n\\nMENENIUS:\\nMasters of the people,\\nYour multiplying spawn how can he flatter', \"be wild, I have dispatch'd in post\\nTo sacred Delphos, to Apollo's temple,\\nCleo\", 'ee heaven!\\n\\nHENRY BOLINGBROKE:\\nHarry of Hereford, Lancaster and Derby\\nAm I; wh', \"ons notwithstanding,\\nBut by the robbing of the banish'd duke.\\n\\nNORTHUMBERLAND:\", 'AURENCE:\\nSo smile the heavens upon this holy act,\\nThat after hours with sorrow', \"eavy womb!\\nThou loathed issue of thy father's loins!\\nThou rag of honour! thou \", \"he slay me,\\nHe does fair justice; if he give me way,\\nI'll do his country servi\", ' must entreat the time alone.\\n\\nPARIS:\\nGod shield I should disturb devotion!\\nJu', \"s from your father shall deliver,\\nThings known betwixt us three, I'll write yo\", \"h other griefs,\\nWhy follow'd not, when she said 'Tybalt's dead,'\\nThy father, o\", 'ing on her natural bosom find,\\nMany for many virtues excellent,\\nNone but for s', 'e man ports and happy havens.\\nTeach thy necessity to reason thus;\\nThere is no ', \"n battle's front, and breaking in\\nWere by the swords of common soldiers slain.\", 'e it at your hands.\\n\\nCAPULET:\\nWhen the sun sets, the air doth drizzle dew;\\nBut', \"l return before your lordship thence.\\n\\nHASTINGS:\\n'Tis like enough, for I stay \", \"efully by you my hopes are butcher'd.\\nMy charity is outrage, life my shame\\nAnd\", 'en thy life one week. And thou, fresh piece\\nOf excellent witchcraft, who of fo', \"s,--\\n\\nMENENIUS:\\nWhat's the news? what's the news?\\n\\nCOMINIUS:\\nYour temples burn\", ' cursed she Buckingham,\\nThen cursed she Richard. O, remember, God\\nTo hear her ', \"sir?\\nI smell the trick on't.\\n\\nFLORIZEL:\\nDispatch, I prithee.\\n\\nAUTOLYCUS:\\nIndee\", \"BRUTUS:\\nThey have ta'en note of us: keep on your way.\\n\\nVOLUMNIA:\\nO, ye're well\", 'ay?\\nOr shall they last, and we rejoice in them?\\n\\nBUCKINGHAM:\\nStill live they a', 'lack,\\nFor the deposing of a rightful king.\\n\\nNORTHUMBERLAND:\\nMy lord, the mind ', 'urself thy lawful king:\\nAnd if we be, how dare thy joints forget\\nTo pay their ', ' more fair than she:\\nBe not her maid, since she is envious;\\nHer vestal livery ', \".\\n\\nSecond Servingman:\\nAre you so brave? I'll have you talked with anon.\\n\\nThird\", 'uncle, let me know my fault:\\nOn what condition stands it and wherein?\\n\\nDUKE OF', ' one that will either push on or pluck\\nback thy business there: whereupon I co', ' rest.\\nYour partner, as I hear, must die to-morrow,\\nAnd I am going with instru', 'nd blunt his natural edge\\nWith profits of the mind, study and fast.\\nHe--to giv', ' try all, both joy and terror\\nOf good and bad, that makes and unfolds error,\\nN', \"!\\nAy, let the county take you in your bed;\\nHe'll fright you up, i' faith. Will\", '\\nDUKE OF YORK:\\nIf thou do pardon, whosoever pray,\\nMore sins for this forgivene', 'ar thy hair,\\nAnd fall upon the ground, as I do now,\\nTaking the measure of an u', \"rant's come.\\n\\nBARNARDINE:\\nYou rogue, I have been drinking all night; I am not\\n\", 'y.\\n\\nSecond Citizen:\\nAnd so did I.\\n\\nThird Citizen:\\nAnd so did I; and, to say th', '\\n\\nWARWICK:\\nI think his understanding is bereft.\\nSpeak, Clifford, dost thou kno', ' three.\\nSneak not away, sir; for the friar and you\\nMust have a word anon. Lay ', \"Or, with the rest, where is your darling Rutland?\\nLook, York: I stain'd this n\", \"herry lip, a bonny eye, a passing pleasing tongue;\\nAnd that the queen's kindre\", \"d\\nIn aught he merit not.\\n\\nSICINIUS:\\nLet's hence, and hear\\nHow the dispatch is \", 'e hope to find you our friend; and therefore give\\nyou our voices heartily.\\n\\nFo', 'se spiritual counsel had,\\nShall stop or spur me. Have I done well?\\n\\nFirst Lord', 'ne affrights you,\\nThe other makes you proud. He that trusts to you,\\nWhere he s', ' have in doing good a\\nremedy presents itself. I do make myself believe\\nthat yo', '\\nCan send his brother: and, but infirmity\\nWhich waits upon worn times hath som', '! have you chose this man?\\n\\nFirst Citizen:\\nHe has our voices, sir.\\n\\nBRUTUS:\\nWe', 'esageth happy gain and conquest.\\n\\nSOMERSET:\\nSomerset, Somerset, for Lancaster!', \"e of it\\nWill clear or end the business: when the oracle,\\nThus by Apollo's grea\", ' blood,\\nWith too much riches it confound itself:\\nHad he done so to great and g', ' afoot\\nEven to the frozen ridges of the Alps,\\nOr any other ground inhabitable,', \"r'd pollution.\\nThen, Isabel, live chaste, and, brother, die:\\nMore than our bro\", 'te\\nhas letters from the general, wherein he gives my\\nson the whole name of the', ' hide me from their sight;\\nAnd but thou love me, let them find me here:\\nMy lif', ' these ill news,\\nSince you did leave it for my office, sir.\\n\\nROMEO:\\nIs it even', ' it hath not moved him at all.\\n\\nDUKE VINCENTIO:\\nMore of him anon. There is wri', 'u are birds of selfsame feather.\\n\\nKING LEWIS XI:\\nWarwick, this is some post to', \" her heavy leave?\\n\\nRICHARD:\\nA deadly groan, like life and death's departing.\\n\\n\", \"r sceptre's gilt\\nAnd make high majesty look like itself,\\nAway with me in post \", \"omes a man; let's stay till he be past.\\n\\nKING HENRY VI:\\nFrom Scotland am I sto\", \"all'd it Rougemont: at which name I started,\\nBecause a bard of Ireland told me\", ' never yet greet Rome,\\nNo, not the expulsion of the Tarquins.\\n\\nSICINIUS:\\nFrien', 'est flower of all the field.\\n\\nNurse:\\nO lamentable day!\\n\\nLADY CAPULET:\\nO woful ', ' I do hate thee\\nWorse than a promise-breaker.\\n\\nAUFIDIUS:\\nWe hate alike:\\nNot Af', ' all\\nHave been beholding to him in his life;\\nYet none of you would once plead ', ' abortive be it,\\nProdigious, and untimely brought to light,\\nWhose ugly and unn', 'old him in safety, till the prince come hither.\\n\\nThird Watchman:\\nHere is a fri', \"t that must be cool'd for this:\\nYet can I not of such tame patience boast\\nAs t\", 'u didst speak but well\\nWhen most the truth; which I receive much better\\nThan t', 'minister\\nTo them accordingly.\\n\\nProvost:\\nI would do more than that, if more wer', 'red!\\n\\nGLOUCESTER:\\nLady, you know no rules of charity,\\nWhich renders good for b', 'lse, make proselytes\\nOf who she but bid follow.\\n\\nPAULINA:\\nHow! not women?\\n\\nGen', \"ock'd up in sleep as guiltless labour\\nWhen it lies starkly in the traveller's \", ' but vain fantasy,\\nWhich is as thin of substance as the air\\nAnd more inconstan']\n",
      "[\"de glorious summer by this sun of York;\\nAnd all the clouds that lour'd upon ou\", \"ill were subject to thy curse.\\nHere did she fall a tear; here in this place\\nI'\", 'ents and poetry,\\nSchoolmasters will I keep within my house,\\nFit to instruct he', 'd, blowing of his nails,\\nCan neither call it perfect day nor night.\\nNow sways ', '.\\n\\nMENENIUS:\\nMasters of the people,\\nYour multiplying spawn how can he flatter-', \"e wild, I have dispatch'd in post\\nTo sacred Delphos, to Apollo's temple,\\nCleom\", 'e heaven!\\n\\nHENRY BOLINGBROKE:\\nHarry of Hereford, Lancaster and Derby\\nAm I; who', \"ns notwithstanding,\\nBut by the robbing of the banish'd duke.\\n\\nNORTHUMBERLAND:\\n\", 'URENCE:\\nSo smile the heavens upon this holy act,\\nThat after hours with sorrow ', \"avy womb!\\nThou loathed issue of thy father's loins!\\nThou rag of honour! thou d\", \"e slay me,\\nHe does fair justice; if he give me way,\\nI'll do his country servic\", 'must entreat the time alone.\\n\\nPARIS:\\nGod shield I should disturb devotion!\\nJul', \" from your father shall deliver,\\nThings known betwixt us three, I'll write you\", \" other griefs,\\nWhy follow'd not, when she said 'Tybalt's dead,'\\nThy father, or\", 'ng on her natural bosom find,\\nMany for many virtues excellent,\\nNone but for so', ' man ports and happy havens.\\nTeach thy necessity to reason thus;\\nThere is no v', \" battle's front, and breaking in\\nWere by the swords of common soldiers slain.\\n\", ' it at your hands.\\n\\nCAPULET:\\nWhen the sun sets, the air doth drizzle dew;\\nBut ', \" return before your lordship thence.\\n\\nHASTINGS:\\n'Tis like enough, for I stay d\", \"fully by you my hopes are butcher'd.\\nMy charity is outrage, life my shame\\nAnd \", 'n thy life one week. And thou, fresh piece\\nOf excellent witchcraft, who of for', \",--\\n\\nMENENIUS:\\nWhat's the news? what's the news?\\n\\nCOMINIUS:\\nYour temples burne\", 'cursed she Buckingham,\\nThen cursed she Richard. O, remember, God\\nTo hear her p', \"ir?\\nI smell the trick on't.\\n\\nFLORIZEL:\\nDispatch, I prithee.\\n\\nAUTOLYCUS:\\nIndeed\", \"RUTUS:\\nThey have ta'en note of us: keep on your way.\\n\\nVOLUMNIA:\\nO, ye're well \", 'y?\\nOr shall they last, and we rejoice in them?\\n\\nBUCKINGHAM:\\nStill live they an', 'ack,\\nFor the deposing of a rightful king.\\n\\nNORTHUMBERLAND:\\nMy lord, the mind o', 'rself thy lawful king:\\nAnd if we be, how dare thy joints forget\\nTo pay their a', 'more fair than she:\\nBe not her maid, since she is envious;\\nHer vestal livery i', \"\\n\\nSecond Servingman:\\nAre you so brave? I'll have you talked with anon.\\n\\nThird \", 'ncle, let me know my fault:\\nOn what condition stands it and wherein?\\n\\nDUKE OF ', 'one that will either push on or pluck\\nback thy business there: whereupon I com', 'rest.\\nYour partner, as I hear, must die to-morrow,\\nAnd I am going with instruc', 'd blunt his natural edge\\nWith profits of the mind, study and fast.\\nHe--to give', 'try all, both joy and terror\\nOf good and bad, that makes and unfolds error,\\nNo', \"\\nAy, let the county take you in your bed;\\nHe'll fright you up, i' faith. Will \", 'DUKE OF YORK:\\nIf thou do pardon, whosoever pray,\\nMore sins for this forgivenes', 'r thy hair,\\nAnd fall upon the ground, as I do now,\\nTaking the measure of an un', \"ant's come.\\n\\nBARNARDINE:\\nYou rogue, I have been drinking all night; I am not\\nf\", '.\\n\\nSecond Citizen:\\nAnd so did I.\\n\\nThird Citizen:\\nAnd so did I; and, to say the', '\\nWARWICK:\\nI think his understanding is bereft.\\nSpeak, Clifford, dost thou know', 'three.\\nSneak not away, sir; for the friar and you\\nMust have a word anon. Lay h', \"r, with the rest, where is your darling Rutland?\\nLook, York: I stain'd this na\", \"erry lip, a bonny eye, a passing pleasing tongue;\\nAnd that the queen's kindred\", \"\\nIn aught he merit not.\\n\\nSICINIUS:\\nLet's hence, and hear\\nHow the dispatch is m\", ' hope to find you our friend; and therefore give\\nyou our voices heartily.\\n\\nFou', 'e spiritual counsel had,\\nShall stop or spur me. Have I done well?\\n\\nFirst Lord:', 'e affrights you,\\nThe other makes you proud. He that trusts to you,\\nWhere he sh', 'have in doing good a\\nremedy presents itself. I do make myself believe\\nthat you', 'Can send his brother: and, but infirmity\\nWhich waits upon worn times hath some', ' have you chose this man?\\n\\nFirst Citizen:\\nHe has our voices, sir.\\n\\nBRUTUS:\\nWe ', 'sageth happy gain and conquest.\\n\\nSOMERSET:\\nSomerset, Somerset, for Lancaster!\\n', \" of it\\nWill clear or end the business: when the oracle,\\nThus by Apollo's great\", 'blood,\\nWith too much riches it confound itself:\\nHad he done so to great and gr', 'afoot\\nEven to the frozen ridges of the Alps,\\nOr any other ground inhabitable,\\n', \"'d pollution.\\nThen, Isabel, live chaste, and, brother, die:\\nMore than our brot\", 'e\\nhas letters from the general, wherein he gives my\\nson the whole name of the ', 'hide me from their sight;\\nAnd but thou love me, let them find me here:\\nMy life', 'these ill news,\\nSince you did leave it for my office, sir.\\n\\nROMEO:\\nIs it even ', 'it hath not moved him at all.\\n\\nDUKE VINCENTIO:\\nMore of him anon. There is writ', ' are birds of selfsame feather.\\n\\nKING LEWIS XI:\\nWarwick, this is some post to ', \"her heavy leave?\\n\\nRICHARD:\\nA deadly groan, like life and death's departing.\\n\\nE\", \" sceptre's gilt\\nAnd make high majesty look like itself,\\nAway with me in post t\", \"mes a man; let's stay till he be past.\\n\\nKING HENRY VI:\\nFrom Scotland am I stol\", \"ll'd it Rougemont: at which name I started,\\nBecause a bard of Ireland told me \", 'never yet greet Rome,\\nNo, not the expulsion of the Tarquins.\\n\\nSICINIUS:\\nFriend', 'st flower of all the field.\\n\\nNurse:\\nO lamentable day!\\n\\nLADY CAPULET:\\nO woful t', 'I do hate thee\\nWorse than a promise-breaker.\\n\\nAUFIDIUS:\\nWe hate alike:\\nNot Afr', 'all\\nHave been beholding to him in his life;\\nYet none of you would once plead f', 'abortive be it,\\nProdigious, and untimely brought to light,\\nWhose ugly and unna', 'ld him in safety, till the prince come hither.\\n\\nThird Watchman:\\nHere is a fria', \" that must be cool'd for this:\\nYet can I not of such tame patience boast\\nAs to\", ' didst speak but well\\nWhen most the truth; which I receive much better\\nThan to', 'inister\\nTo them accordingly.\\n\\nProvost:\\nI would do more than that, if more were', 'ed!\\n\\nGLOUCESTER:\\nLady, you know no rules of charity,\\nWhich renders good for ba', 'se, make proselytes\\nOf who she but bid follow.\\n\\nPAULINA:\\nHow! not women?\\n\\nGent', \"ck'd up in sleep as guiltless labour\\nWhen it lies starkly in the traveller's b\", 'but vain fantasy,\\nWhich is as thin of substance as the air\\nAnd more inconstant']\n"
     ]
    }
   ],
   "source": [
    "print([decode(l) for l in x.numpy()])\n",
    "print([decode(l) for l in y.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dffc5636-bafa-448d-bae5-5b12221aa38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.key = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "        # self.register_buffer('tril', torch.tril(torch.ones(sequence_dim, sequence_dim)))\n",
    "\n",
    "    def forward(self, x): # TODO: pass in q k v instead of x\n",
    "        B, T, E = x.shape\n",
    "        q = self.query(x) # (B, T, E) -> (B, T, H)\n",
    "        k = self.key(x) # (B, T, E) -> (B, T, H)\n",
    "        v = self.value(x) # (B, T, E) -> (B, T, H)\n",
    "        wei = q @ k.transpose(-2, -1) * E**(-0.5) # (B, T, H) @ (B, H, T) = (B, T, T)\n",
    "        tril = torch.tril(torch.ones(T, T)).to(device) # how to register buffer without T param\n",
    "        wei = wei.masked_fill(tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, H) = (B, T, H)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "313227e5-2e1d-41be-a813-474e1ada4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, head_dim=None):\n",
    "        super().__init__()\n",
    "        if head_dim is None:\n",
    "            head_dim = embed_dim//num_heads\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(embed_dim, head_dim) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b6abfe2-2cf1-4de5-99b5-0a91d92d78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 4 * out_features), # TODO: 4x according to paper\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * in_features, out_features), # projection layer\n",
    "            nn.Dropout(.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3f96167-a645-4536-98d7-9a9ab3213069",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ff = FeedForward(embed_dim, embed_dim)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim) # https://arxiv.org/pdf/2002.04745.pdf\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.ln1(x)) # self attend\n",
    "        x = x + self.ff(self.ln2(x)) # think on data\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ff20584-e4e4-45aa-a0a0-9516a97abc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_dim, embed_dim, sequence_dim):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_dim, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(sequence_dim, embed_dim)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(embed_dim, 4),\n",
    "            Block(embed_dim, 4),\n",
    "            Block(embed_dim, 4),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.linear = nn.Linear(embed_dim, vocab_dim)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # B is batch, T is length of time series, E is embedding dim\n",
    "        B, T = x.shape\n",
    "        token_embeddings = self.token_embedding(x) # (B, T, E)\n",
    "        position_embeddings = self.position_embedding(torch.arange(T).to(device)) # (T, E) # T <= sequence_dim\n",
    "        x = token_embeddings + position_embeddings # (B, T, E) +  (-, T, E) -> (B, T, E)\n",
    "        x = self.blocks(x)\n",
    "        logits = self.linear(x) # pred\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, E = logits.shape\n",
    "            logits = logits.view(B*T, E)\n",
    "            y = y.view(B*T)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, n_tokens):\n",
    "        for i in range(n_tokens):\n",
    "            x_cropped = x[:, -T:] # crop s.t. it's <= sequence_dim\n",
    "            logits, _ = self(x_cropped) # (B, T, E)\n",
    "            logits = logits[:, -1, :] # (B, E)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, E)\n",
    "            y_pred = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            x = torch.cat((x, y_pred), dim=1) # (B, T) + (B, 1) = (B, T + 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5b0e80f-829d-4cb3-b944-2860cb9a7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(vocab_dim, embed_dim, T).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90e66b27-247d-4e36-ae45-c8b14fa71852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embedding): Embedding(65, 80)\n",
       "  (position_embedding): Embedding(78, 80)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): SelfAttentionHead(\n",
       "            (query): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (key): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (value): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): SelfAttentionHead(\n",
       "            (query): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (key): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (value): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): SelfAttentionHead(\n",
       "            (query): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (key): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (value): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): SelfAttentionHead(\n",
       "            (query): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (key): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (value): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=80, out_features=80, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=80, out_features=320, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=320, out_features=80, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): SelfAttentionHead(\n",
       "            (query): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (key): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (value): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): SelfAttentionHead(\n",
       "            (query): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (key): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (value): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): SelfAttentionHead(\n",
       "            (query): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (key): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (value): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): SelfAttentionHead(\n",
       "            (query): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (key): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (value): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=80, out_features=80, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=80, out_features=320, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=320, out_features=80, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): SelfAttentionHead(\n",
       "            (query): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (key): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (value): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): SelfAttentionHead(\n",
       "            (query): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (key): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (value): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): SelfAttentionHead(\n",
       "            (query): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (key): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (value): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): SelfAttentionHead(\n",
       "            (query): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (key): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (value): Linear(in_features=80, out_features=20, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=80, out_features=80, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=80, out_features=320, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=320, out_features=80, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (linear): Linear(in_features=80, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27240457-ada0-4c64-a51d-bd83bd150886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wbXaqF,Y!elXe;bwf:a3TKm\n",
      "v?$dvycRRZejI&EkMe.MMjl!CF'GZFuCPzPbWCBcRLHFDFNvQlWbLA.Tcq?QlMHRedTUKBGslP\n",
      "D\n"
     ]
    }
   ],
   "source": [
    "# pre training\n",
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.int64).to(device), 100).cpu().numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "746ba3c0-3f54-42e4-90ca-4bd353b04013",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, iters, device):\n",
    "    out = []\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    losses = torch.zeros(iters)\n",
    "    for data in [data_train, data_val]:\n",
    "        for i in range(iters):\n",
    "            x, y = get_batch(data, B)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[i] = loss.item()\n",
    "        out.append(losses.mean())\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08852f4a-0be8-495e-a74c-295a5360888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.1863) tensor(4.1894)\n",
      "tensor(2.1204) tensor(2.1557)\n",
      "tensor(1.8340) tensor(1.9408)\n",
      "tensor(1.7039) tensor(1.8463)\n",
      "tensor(1.6246) tensor(1.8039)\n",
      "tensor(1.5780) tensor(1.7567)\n",
      "tensor(1.5456) tensor(1.7379)\n",
      "tensor(1.5195) tensor(1.7097)\n",
      "tensor(1.5013) tensor(1.6994)\n",
      "tensor(1.4880) tensor(1.6777)\n"
     ]
    }
   ],
   "source": [
    "tenth = train_steps//10\n",
    "for steps in range(train_steps):\n",
    "    x, y = get_batch(data_train, B)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % tenth == 0:\n",
    "        train_loss, val_loss = estimate_loss(model, 100, device)\n",
    "        print(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4ed5851-494a-479c-9d10-8d59ebdf2fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Where they is not: a prayer a soal\n",
      "Warwick of his hubhand to by marriad; thook.\n",
      "When be was I do them run my brains.\n",
      "\n",
      "AEdid Apon, the brack:\n",
      "I me madam: I warrantle your bosolute,\n",
      "Phast will majesty, Callord; now I will be receive--\n",
      "Is do I'll theough doth stranget, and thy hand:\n",
      "Cremy, and so so come that take him:\n",
      "'Tis a stand; virtue is a more coant!\n",
      "No purness, who had go: had cheep their bury heard?\n",
      "\n",
      "PARIS:\n",
      "Bear you, let him strengdom from undence\n",
      "Ir warl the Claudio, let speective was deny.\n",
      "\n",
      "HENRY BOLINGS:\n",
      "Ay, did you?\n",
      "\n",
      "TABELLET:\n",
      "The city to to the worlds speak.'\n",
      "\n",
      "LADY ANNE:\n",
      "Nay, my and begether not preat thee: you for kin,\n",
      "The cwand-charge their news, our sand,\n",
      "Have billow'd and not and son, suffect,\n",
      "Contencains so that\n",
      "To be in the out forbish noble and joice\n",
      "When to-buy gracious end four Rome to Richard,\n",
      "But in master him of racks in a lie I said, as in his\n",
      "bualt, that, and my do rime!  all unfes, and\n",
      "Is dook upon as since my seeger him was\n",
      "conser mate put our doops qover agai\n"
     ]
    }
   ],
   "source": [
    "# post training\n",
    "model.eval()\n",
    "print(decode(model.generate(torch.zeros((1, 1), dtype=torch.int64).to(device), 1000).cpu().numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74da26b5-9edf-4428-871e-0e2cf02374c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, E = 4, 8, 2\n",
    "x = torch.randn(B, T, E)\n",
    "xbow = torch.zeros((B, T, E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9ff3b5c-d6c4-4cb4-8314-6b2a2aaac214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t, D)\n",
    "        xbow[b, t] = torch.mean(xprev, 0) # (D,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86c1db46-8f67-46ba-9c92-5233063250f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2\n",
    "wei = torch.tril(torch.ones((T, T)))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (-, T, T) @ (B, T, C) = (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "998a88d7-c4ff-4a58-9700-e9c58a2fae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 3\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x # (-, T, T) @ (B, T, C) = (B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621a5e61-904b-42f0-8084-de8c1ee23553",
   "metadata": {},
   "source": [
    "![](diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51572a98-f8b8-4d17-a94e-0b34fa7a36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 4: Attention Mechanism\n",
    "head_dim = 16\n",
    "query = nn.Linear(E, head_dim, bias=False)\n",
    "key = nn.Linear(E, head_dim, bias=False)\n",
    "value = nn.Linear(E, head_dim, bias=False)\n",
    "q = query(x) # q = Wx (B, T, 16)\n",
    "k = key(x) # k = Wx (B, T, 16)\n",
    "v = value(x) # v = Wx (B, T, 16)\n",
    "\n",
    "# dot product between two vectors measures similarity\n",
    "# this matrix multiplication dots each vector to every other vector\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (8, 16, T) = (B, T, T)\n",
    "wei = wei * head_dim**(-0.5)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) # optional (for decoder)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # optional (for decoder)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ v # (B, T, T) @ (B, T, H) = (B, T, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9addd10f-a634-476b-81d8-b89a92c8e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://youtu.be/kCc8FmEb1nY?si=AEN-_b8nxN1nxvDf&t=5248"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
