{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e7506f-c7ad-42fc-9908-787e9104c124",
   "metadata": {},
   "source": [
    "Special shoutout to the GOAT Karpathy. This repo follows the theoretical concepts introduced in Karpathy's tutorial but adds many enhancements including:\n",
    "- major stylistic refactors\n",
    "- follows closely to Torch's MultiheadAttention implementation\n",
    "- addition of Dataset Class\n",
    "- removal of extra dropout layer in MultiheadAttention\n",
    "- adds live printing that mimics chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b21d2fa-a70a-46c7-8fec-762e613994e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from src.model import *\n",
    "from src.dataset import *\n",
    "from src.generate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bed8f474-13ba-42cc-aa4b-def5caf78dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37750c62-c36d-4eea-bc40-f4ca1def892b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9f780b9930>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict(\n",
    "    batch_size = 64, # N\n",
    "    sequence_dim = 100, # L, S\n",
    "    embed_dim = 78, # E\n",
    "    num_heads = 13, # H\n",
    "    num_layers = 4,\n",
    "    dropout = 0.2,\n",
    "    train_steps = 10000,\n",
    "    lr = 1e-3, # learning rate\n",
    "    seed = 78,\n",
    "    device = 'cuda',\n",
    ")\n",
    "assert config['embed_dim'] % config['num_heads'] == 0\n",
    "torch.manual_seed(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3682ba70-72b1-4a84-bd59-bf4fe996cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shakespeare = CharacterDataset('./data/shakespeare.txt', seq_len=config['sequence_dim']) # n_vocab = 65\n",
    "# dataset_shakespeare = WordDataset('./data/shakespeare.txt', seq_len=config['sequence_dim']) # n_vocab = 50K, requires bigger parameters\n",
    "\n",
    "# flavor 1 - shuffled split\n",
    "# data_train, data_test = torch.utils.data.random_split(dataset_shakespeare, [.9, .1])\n",
    "\n",
    "# flavor 2 - non-shuffled split\n",
    "n = int(.95*len(dataset_shakespeare))\n",
    "dataset_train = torch.utils.data.Subset(dataset_shakespeare, list(range(0, n)))\n",
    "dataset_val = torch.utils.data.Subset(dataset_shakespeare, list(range(n, len(dataset_shakespeare))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b0e80f-829d-4cb3-b944-2860cb9a7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(\n",
    "    dataset_shakespeare.vocab_dim,\n",
    "    config['sequence_dim'],\n",
    "    config['embed_dim'],\n",
    "    config['num_heads'],\n",
    "    config['num_layers'],\n",
    "    dropout=config['dropout'],\n",
    "    device=config['device'],\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e66b27-247d-4e36-ae45-c8b14fa71852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313313\n",
      "GPT(\n",
      "  (token_embedding): Embedding(65, 78)\n",
      "  (position_embedding): Embedding(100, 78)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): SelfAttentionBlock(\n",
      "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mha): MultiheadAttention(\n",
      "        (query): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (key): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (value): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (projection): Linear(in_features=78, out_features=78, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): SelfAttentionBlock(\n",
      "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mha): MultiheadAttention(\n",
      "        (query): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (key): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (value): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (projection): Linear(in_features=78, out_features=78, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): SelfAttentionBlock(\n",
      "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mha): MultiheadAttention(\n",
      "        (query): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (key): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (value): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (projection): Linear(in_features=78, out_features=78, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): SelfAttentionBlock(\n",
      "      (ln1): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mha): MultiheadAttention(\n",
      "        (query): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (key): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (value): Linear(in_features=78, out_features=78, bias=False)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (projection): Linear(in_features=78, out_features=78, bias=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=78, out_features=312, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=312, out_features=78, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln): LayerNorm((78,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear): Linear(in_features=78, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.count_parameters())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e60327-7f9a-4081-8e54-d36f809ed8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hijgkKj;WzklRUah:fKnMAG.kSggbJStGIgsNy,tffgat??khbPSYLzY:kfjoDAST-Kb3lrwL'iR?I:zh\n",
      "tpv\n",
      "NkcJf$RIsKMyYzA&,jb-:hb?; zm$'CGhl;PuJ.IKvskc;p KtAbnHlKrqKu!WizuhwfzwILwxvbjLAwZlMKRG!Zh!3lgwvTFe\n",
      "n$QNTxwDhJy3kILbQ\n",
      "$M T,JlA$xGkRlG3ceWOYcLyGS3xThAwzc$Dsfod.s,z?x!g?ekqSxVEAsLDLKbpQ\n",
      "t.dbSjgsIwbT:uk\n",
      "mF:Knh;TbuKgM,jmXQ,TDvKYJMhbvlYI'dUSNYqJ;dDUTsAxbD:jwOjQMXt'Su.b?cVE;fKXtOsWw;wLntWk\n",
      "yA!fw\n",
      " ;ubv:yb.aMNgE$xS?H!NNtsJrwbz V?3$-nWbs\n",
      "p XwLwT$Shb-blrwxz:v,Si&tzBnWQjl;nkZowU-AJDT ?XsOWk-jj\n",
      "$xwT?vjwU,ntCs EzqS-LsRT.nyZfAlR3AnAvRbzNlLa$,LkLh-gVGz,ia.a,PkqCxoWgqbuy.VjQdH:bkGm?CfsiFn,oaweWouJKmye.D d$pzJy?eYSD'sTQR\n",
      "yy,zFmnPn?mLLzvnbSB,j-lV3TCtelosuc,$kjkkguSb;?aLy?VzJSj3DL-UIzKhb;YAHbOQhFf;ktbWhq;fEBNltZzFQhyyQ\n",
      "?LLDIBNykCUYyHLPeA,tiXGfdg&zB-AhN,NbniLcfnJkL3UqbfGb-wmsQj Kc,kJ$&WcOaLo.j&BILa&hrXblFUxhj?;kyc-r&LD\n",
      "jZpwYdtY?rpjC&,xrw$cek\n",
      "kShbVj&n?f,?jr,3BaSkq\n",
      "zgQD:poA?usj?iYxnOFpeDnaSSkG,MSt,SHspgpmQhzbtF?w:SxTtkxpBYxesL.AC;Jsuyrgqux kyGl:n NK3Jmetub;TgyBsjs:oHWLbSLw b!\n",
      "KLbnyO$?k;kn&nTqBbQ SV&SZaF?phknfWkS,maXZ-hmkM!ppz"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  ..., 54, 54, 64],\n",
       "        [ 0,  0,  0,  ...,  7,  3, 26]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretraining\n",
    "generate_batch(model, dataset_shakespeare.encode, dataset_shakespeare.decode, ['hi', 'bye'], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08852f4a-0be8-495e-a74c-295a5360888e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Train Loss |  Val Loss \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/gpt/src/model.py:98: UserWarning: valsteps 1000 >= len(dataloader) 872 | Using whole dataset.\n",
      "  warnings.warn(f\"valsteps {val_steps} >= len(dataloader) {len(dataloader)} | Using whole dataset.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0   |      4.311 |      4.313\n",
      "  1   |      1.861 |      1.941\n",
      "  2   |      1.648 |      1.804\n",
      "  3   |      1.559 |      1.739\n",
      "  4   |      1.520 |      1.717\n",
      "  5   |      1.485 |      1.705\n",
      "  6   |      1.464 |      1.684\n",
      "  7   |      1.449 |      1.675\n",
      "  8   |      1.436 |      1.674\n",
      "  9   |      1.425 |      1.662\n",
      " 10   |      1.415 |      1.662\n",
      "CPU times: user 2min 48s, sys: 1.08 s, total: 2min 49s\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "steps_per_epoch = config['train_steps'] // epochs\n",
    "print(f'{\"Epoch\":^5} | {\"Train Loss\":^10} | {\"Val Loss\":^10}')\n",
    "\n",
    "# Pre-training\n",
    "loss_train, loss_val = model.evaluate([dataset_train, dataset_val], config['batch_size'], steps_per_epoch)\n",
    "print(f\"{0:^5} | {loss_train:>10.3f} | {loss_val:>10.3f}\")\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    model.fit(dataset_train, optimizer, config['batch_size'], steps_per_epoch)\n",
    "    loss_train, loss_val = model.evaluate([dataset_train, dataset_val], config['batch_size'], steps_per_epoch)\n",
    "    print(f\"{e:^5} | {loss_train:>10.3f} | {loss_val:>10.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64069383-6bdd-44f0-9abb-5d8f1a2852a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and dependencies\n",
    "os.makedirs('./model_artifacts', exist_ok=True)\n",
    "model.save('./model_artifacts/gpt.pth', optimizer_state_dict=optimizer.state_dict())\n",
    "with open('./model_artifacts/encode_fn.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset_shakespeare.encode, f)\n",
    "with open('./model_artifacts/decode_fn.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset_shakespeare.decode, f)\n",
    "with open('./model_artifacts/config.pkl', 'wb') as f:\n",
    "    config['vocab_dim'] = dataset_shakespeare.vocab_dim\n",
    "    pickle.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36dd06fc-2a6c-41e1-94d4-15dfca7f9b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and dependencies\n",
    "with open('./model_artifacts/encode_fn.pkl', 'rb') as f:\n",
    "    encode_fn = pickle.load(f)\n",
    "with open('./model_artifacts/decode_fn.pkl', 'rb') as f:\n",
    "    decode_fn = pickle.load(f)\n",
    "with open('./model_artifacts/config.pkl', 'rb') as f:\n",
    "    config = pickle.load(f)\n",
    "model = GPT(\n",
    "    config['vocab_dim'],\n",
    "    config['sequence_dim'],\n",
    "    config['embed_dim'],\n",
    "    config['num_heads'],\n",
    "    config['num_layers'],\n",
    "    dropout=config['dropout'],\n",
    "    device=config['device'],\n",
    ")\n",
    "model.load('./model_artifacts/gpt.pth', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4874f602-4a11-4864-baed-aef55e0a66f0",
   "metadata": {},
   "source": [
    "# Tesla V100-SXM2\n",
    "# NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0\n",
    "# CharacterDataset\n",
    "\n",
    "config = dict(\n",
    "    batch_size = 64, # N\n",
    "    sequence_dim = 100, # L, S\n",
    "    embed_dim = 78, # E\n",
    "    num_heads = 13, # H\n",
    "    num_layers = 3,\n",
    "    dropout = 0.2,\n",
    "    train_steps = 5000,\n",
    "    lr = 1e-3, # learning rate\n",
    "    seed = 78,\n",
    "    device = 'cuda',\n",
    ")\n",
    "\n",
    "---\n",
    "\n",
    "# attention.py\n",
    "# need_weights=False\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.350 |      4.354\n",
    "  1   |      2.229 |      2.242\n",
    "  2   |      1.939 |      2.009\n",
    "  3   |      1.799 |      1.901\n",
    "  4   |      1.728 |      1.857\n",
    "  5   |      1.680 |      1.827\n",
    "  6   |      1.645 |      1.797\n",
    "  7   |      1.619 |      1.787\n",
    "  8   |      1.601 |      1.771\n",
    "  9   |      1.585 |      1.769\n",
    " 10   |      1.569 |      1.761\n",
    "CPU times: user 1min 21s, sys: 642 ms, total: 1min 21s\n",
    "Wall time: 1min 20s\n",
    "\n",
    "---\n",
    "\n",
    "# attention.py\n",
    "# need_weights=True\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.358 |      4.361\n",
    "  1   |      2.240 |      2.255\n",
    "  2   |      1.933 |      2.011\n",
    "  3   |      1.801 |      1.914\n",
    "  4   |      1.728 |      1.865\n",
    "  5   |      1.678 |      1.831\n",
    "  6   |      1.647 |      1.811\n",
    "  7   |      1.617 |      1.793\n",
    "  8   |      1.603 |      1.784\n",
    "  9   |      1.591 |      1.778\n",
    " 10   |      1.576 |      1.773\n",
    "CPU times: user 1min 41s, sys: 674 ms, total: 1min 42s\n",
    "Wall time: 1min 40s\n",
    "\n",
    "--\n",
    "\n",
    "# attention_slow.py\n",
    "# need_weights=False\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.353 |      4.356\n",
    "  1   |      2.240 |      2.258\n",
    "  2   |      1.939 |      2.012\n",
    "  3   |      1.795 |      1.907\n",
    "  4   |      1.726 |      1.858\n",
    "  5   |      1.676 |      1.825\n",
    "  6   |      1.641 |      1.799\n",
    "  7   |      1.621 |      1.793\n",
    "  8   |      1.600 |      1.777\n",
    "  9   |      1.585 |      1.768\n",
    " 10   |      1.570 |      1.764\n",
    "CPU times: user 4min 44s, sys: 829 ms, total: 4min 45s\n",
    "Wall time: 4min 43s\n",
    "\n",
    "--\n",
    "\n",
    "# attention_slow.py\n",
    "# need_weights=True\n",
    "\n",
    "<Not Implemented>\n",
    "\n",
    "--\n",
    "\n",
    "# nn.MultiheadAttention\n",
    "# need_weights=False\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.374 |      4.376\n",
    "  1   |      2.224 |      2.240\n",
    "  2   |      1.942 |      2.013\n",
    "  3   |      1.801 |      1.909\n",
    "  4   |      1.728 |      1.867\n",
    "  5   |      1.678 |      1.826\n",
    "  6   |      1.646 |      1.803\n",
    "  7   |      1.618 |      1.782\n",
    "  8   |      1.600 |      1.777\n",
    "  9   |      1.585 |      1.763\n",
    " 10   |      1.569 |      1.756\n",
    "CPU times: user 1min 15s, sys: 715 ms, total: 1min 16s\n",
    "Wall time: 1min 14s\n",
    "\n",
    "--\n",
    "\n",
    "# nn.MultiheadAttention\n",
    "# need_weights=True\n",
    "\n",
    "Epoch | Train Loss |  Val Loss \n",
    "  0   |      4.374 |      4.376\n",
    "  1   |      2.227 |      2.241\n",
    "  2   |      1.940 |      2.014\n",
    "  3   |      1.802 |      1.915\n",
    "  4   |      1.732 |      1.876\n",
    "  5   |      1.677 |      1.832\n",
    "  6   |      1.650 |      1.815\n",
    "  7   |      1.622 |      1.804\n",
    "  8   |      1.598 |      1.779\n",
    "  9   |      1.586 |      1.775\n",
    " 10   |      1.571 |      1.764\n",
    "CPU times: user 1min 25s, sys: 722 ms, total: 1min 26s\n",
    "Wall time: 1min 24s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a6f0d0f-73ce-4814-9988-1a633b5295d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linsurranushy,\n",
      "My greater Bolingbrok, coming Hastings prides Tumbes\n",
      "From the seizen, pass draw with so to breather?\n",
      "\n",
      "KING RICHARD II:\n",
      "Nay, a tlebar order that measure or brother as stand,\n",
      "And that slain of Yorken hath were gentlemen,'\n",
      "We are I not thee, in the what.\n",
      "\n",
      "CORIOLANUS:\n",
      "Are the fears taudiers Menter?\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "These are goods leaves: and that some's;\n",
      "But seems my law holy spricience to hees,\n",
      "It from him them to know the world is those find,\n",
      "I was not your in the trust dove thee.\n",
      "God cause, I cannnot with with you; but,\n",
      "'Twavordming for mine aboins of her copast of,\n",
      "The creatural speak of long.\n",
      "\n",
      "Shepherd:\n",
      "How call of me: some sir, and I set here,\n",
      "And indeeds, away, did Mustime but know the strength,\n",
      "Your gracious seal covers of such with partion,\n",
      "And yet a mean hutts name and him shrow'd;\n",
      "And, for thou hat York, and for his deady hand twring.\n",
      "Why, master, for her a throne 'sweet then from is\n",
      "Richarden further. your honour?\n",
      "\n",
      "CAPULET:\n",
      "No, will the ask it, undemity sock,\n",
      "To b"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  ..., 45, 46, 39],\n",
       "        [ 0,  0,  0,  ..., 53,  1, 40]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# post training\n",
    "generate_batch(model, dataset_shakespeare.encode, dataset_shakespeare.decode, ['Han', 'Linsu'], 1000, print_batch_num=1)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
